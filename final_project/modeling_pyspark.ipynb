{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.getcwd()\n",
    "temporary_folder = os.path.join(os.getcwd(), \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files():\n",
    "# Unzip file on a temporary folder\n",
    "    if os.path.exists(temporary_folder):\n",
    "        shutil.rmtree(temporary_folder)\n",
    "        \n",
    "    if not os.path.exists(temporary_folder):\n",
    "        os.makedirs(temporary_folder)\n",
    "        \n",
    "    local_file_name = os.path.join(base_folder, \"training_dataset\", \"trainingandtestdata.zip\")\n",
    "    with zipfile.ZipFile(local_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temporary_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweets\n",
    "\n",
    "The following function prepares the tweet by:\n",
    "\n",
    "* Extracting the text from HTML (for the training dataset provided, we already have the text, but we want to avoid using any HTML tag for classification\n",
    "* Converting all words to lower case\n",
    "* Replacing any URL with \"URL\" constant (to enable the removal of them on a further step)\n",
    "* Replacing any tagging of users with \"USERTAGGING\" (to enable the removal of them in a further step)\n",
    "* Removing any \"#\" from hashtags\n",
    "* Removing punctuation (has little or no weight on classification as it can be used for both intentions)\n",
    "* And finally, removing words and punctuation that has little or no weight on classification (and can even create biases):\n",
    "    * Stop words: set of common words that are used doesn't matter the intenttion (things like it, that, a, the)\n",
    "    * Remove the two constants that we used to replace user tagging and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(tweet):\n",
    "# Cleansing tweet\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords \n",
    "    from string import punctuation \n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    terms_to_remove = set(stopwords.words(\"english\") + [\"USERTAGGING\",\"URL\"])\n",
    "    tweet = BeautifulSoup(tweet, 'html.parser').get_text() # Extracts text from HTML (just in case!)\n",
    "    tweet = tweet.lower() # Converts text to lower-case\n",
    "    tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet) # Replces URLs by URL constan\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"USERTAGGING\", tweet) # Replaces usernames by USERTAGGING constant \n",
    "    tweet = re.sub(r\"#([^\\s]+)\", r\"\\1\", tweet) # Removes the # in #hashtag\n",
    "    for p in punctuation: \n",
    "        tweet = tweet.replace(p, \"\") # Removes punctiation\n",
    "    tweet = word_tokenize(tweet) # Creates a list of words\n",
    "    words = \"\"\n",
    "    for each_word in tweet:\n",
    "        if each_word not in terms_to_remove:\n",
    "            words = words + \" \" + each_word\n",
    "    # return [word for word in tweet if word not in terms_to_remove]\n",
    "    return words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session, load the dataset into a Spark DataFrame and then adjust column names\n",
    "from pyspark.sql import SparkSession, functions\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Training Twitter Sentiment Analysis\").getOrCreate()\n",
    "training_data = spark.read.load(\n",
    "    \"tmp/training.1600000.processed.noemoticon.csv\",\n",
    "    format=\"csv\")\n",
    "training_data = training_data.withColumnRenamed(\"_c0\", \"label\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"tweet_id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"query\") \\\n",
    "    .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "    .withColumnRenamed(\"_c5\", \"tweet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are loading just a bunch of lines locally. On the server we will use the whole dataset to train the model\n",
    "sample_size = 20000\n",
    "training_data = training_data.sample(sample_size / training_data.count())\n",
    "\n",
    "training_data = training_data.select(functions.col(\"label\"), functions.col(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               tweet|      tweet_cleansed|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|blagh class at 8 ...|blagh class 8 tom...|\n",
      "|    0|@tea oh! i'm so s...|oh im sorry didnt...|\n",
      "|    0|A bad nite for th...|bad nite favorite...|\n",
      "|    0|@ColinDeMar Far t...|   far way rail tips|\n",
      "|    0|@rcompo RACHEL! h...|rachel hang outag...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the cleansing UDF for tweet column\n",
    "udf_cleansing = functions.udf(cleansing)\n",
    "training_data = training_data.withColumn(\"tweet_cleansed\", udf_cleansing(functions.col(\"tweet\")))\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "The following code snippet creates a list of every remaining word (after cleansing) that will be used to build the features for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|               tweet|      tweet_cleansed|               words|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|blagh class at 8 ...|blagh class 8 tom...|[blagh, class, 8,...|\n",
      "|    0|@tea oh! i'm so s...|oh im sorry didnt...|[oh, im, sorry, d...|\n",
      "|    0|A bad nite for th...|bad nite favorite...|[bad, nite, favor...|\n",
      "|    0|@ColinDeMar Far t...|   far way rail tips|[far, way, rail, ...|\n",
      "|    0|@rcompo RACHEL! h...|rachel hang outag...|[rachel, hang, ou...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_cleansed\", outputCol=\"words\")\n",
    "training_data = tokenizer.transform(training_data)\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF\n",
    "\n",
    "The following code snippet creates the features, which means a list of all terms available on all \"documents\" with a \"tag\" indicating its presence or not on the specific \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               tweet|      tweet_cleansed|               words|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|blagh class at 8 ...|blagh class 8 tom...|[blagh, class, 8,...|(262144,[8254,291...|\n",
      "|    0|@tea oh! i'm so s...|oh im sorry didnt...|[oh, im, sorry, d...|(262144,[18184,31...|\n",
      "|    0|A bad nite for th...|bad nite favorite...|[bad, nite, favor...|(262144,[9664,623...|\n",
      "|    0|@ColinDeMar Far t...|   far way rail tips|[far, way, rail, ...|(262144,[51471,16...|\n",
      "|    0|@rcompo RACHEL! h...|rachel hang outag...|[rachel, hang, ou...|(262144,[3386,178...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "training_data = hashingTF.transform(training_data)\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|               tweet|      tweet_cleansed|               words|            features|labelIndex|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|blagh class at 8 ...|blagh class 8 tom...|[blagh, class, 8,...|(262144,[8254,291...|       1.0|\n",
      "|    0|@tea oh! i'm so s...|oh im sorry didnt...|[oh, im, sorry, d...|(262144,[18184,31...|       1.0|\n",
      "|    0|A bad nite for th...|bad nite favorite...|[bad, nite, favor...|(262144,[9664,623...|       1.0|\n",
      "|    0|@ColinDeMar Far t...|   far way rail tips|[far, way, rail, ...|(262144,[51471,16...|       1.0|\n",
      "|    0|@rcompo RACHEL! h...|rachel hang outag...|[rachel, hang, ou...|(262144,[3386,178...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a numeric index for the labels\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "model = stringIndexer.fit(training_data)\n",
    "training_data = model.transform(training_data)\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test\n",
    "training, test = training_data.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "#Naive bayes\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"labelIndex\", predictionCol=\"NB_pred\",\n",
    "                probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "nbModel = nb.fit(training)\n",
    "cv = nbModel.transform(test)\n",
    "total = cv.count()\n",
    "correct = cv.where(cv['labelIndex'] == cv['NB_pred']).count()\n",
    "accuracy = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 10024 \n",
      "Correct: 7159 \n",
      "Accuracy: 0.7141859537110934\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nTotal:\", total, \n",
    "    \"\\nCorrect:\", correct, \n",
    "    \"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+-------+\n",
      "|label|               tweet|labelIndex|NB_pred|\n",
      "+-----+--------------------+----------+-------+\n",
      "|    0| 1 dinger in and ...|       1.0|    0.0|\n",
      "|    0| @shanedawson i'm...|       1.0|    1.0|\n",
      "|    0| @thesmartmama I ...|       1.0|    1.0|\n",
      "|    0| I caught up. I W...|       1.0|    1.0|\n",
      "|    0| I guess I'm no S...|       1.0|    1.0|\n",
      "|    0| I need another w...|       1.0|    1.0|\n",
      "|    0| Just finished th...|       1.0|    1.0|\n",
      "|    0| Misses someone  ...|       1.0|    1.0|\n",
      "|    0| My stomach is ac...|       1.0|    1.0|\n",
      "|    0| Only @ninapolita...|       1.0|    0.0|\n",
      "|    0| That guys superr...|       1.0|    1.0|\n",
      "|    0| Thats cause the ...|       1.0|    1.0|\n",
      "|    0| Too Emotional ri...|       1.0|    1.0|\n",
      "|    0| a kid at my scho...|       1.0|    1.0|\n",
      "|    0|     called in again|       1.0|    1.0|\n",
      "|    0| cant afford to s...|       1.0|    0.0|\n",
      "|    0| damn, Kayley doe...|       1.0|    1.0|\n",
      "|    0| don't go to jail...|       1.0|    1.0|\n",
      "|    0| finished my ice ...|       1.0|    0.0|\n",
      "|    0| going home. Miss...|       1.0|    1.0|\n",
      "+-----+--------------------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv.select(\"label\", \"tweet\", \"labelIndex\", \"NB_pred\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment_spark\")\n",
    "nbModel.write().overwrite().save(model_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary folder\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.getcwd()\n",
    "temporary_folder = os.path.join(os.getcwd(), \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files():\n",
    "# Unzip file on a temporary folder\n",
    "    if os.path.exists(temporary_folder):\n",
    "        shutil.rmtree(temporary_folder)\n",
    "        \n",
    "    if not os.path.exists(temporary_folder):\n",
    "        os.makedirs(temporary_folder)\n",
    "        \n",
    "    local_file_name = os.path.join(base_folder, \"training_dataset\", \"trainingandtestdata.zip\")\n",
    "    with zipfile.ZipFile(local_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temporary_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweets\n",
    "\n",
    "The following function prepares the tweet by:\n",
    "\n",
    "* Extracting the text from HTML (for the training dataset provided, we already have the text, but we want to avoid using any HTML tag for classification\n",
    "* Converting all words to lower case\n",
    "* Replacing any URL with \"URL\" constant (to enable the removal of them on a further step)\n",
    "* Replacing any tagging of users with \"USERTAGGING\" (to enable the removal of them in a further step)\n",
    "* Removing any \"#\" from hashtags\n",
    "* Removing punctuation (has little or no weight on classification as it can be used for both intentions)\n",
    "* And finally, removing words and punctuation that has little or no weight on classification (and can even create biases):\n",
    "    * Stop words: set of common words that are used doesn't matter the intenttion (things like it, that, a, the)\n",
    "    * Remove the two constants that we used to replace user tagging and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(tweet):\n",
    "# Cleansing tweet\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords \n",
    "    from string import punctuation \n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    terms_to_remove = set(stopwords.words(\"english\") + [\"USERTAGGING\",\"URL\"])\n",
    "    tweet = BeautifulSoup(tweet, 'html.parser').get_text() # Extracts text from HTML (just in case!)\n",
    "    tweet = tweet.lower() # Converts text to lower-case\n",
    "    tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet) # Replces URLs by URL constan\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"USERTAGGING\", tweet) # Replaces usernames by USERTAGGING constant \n",
    "    tweet = re.sub(r\"#([^\\s]+)\", r\"\\1\", tweet) # Removes the # in #hashtag\n",
    "    for p in punctuation: \n",
    "        tweet = tweet.replace(p, \"\") # Removes punctiation\n",
    "    tweet = word_tokenize(tweet) # Creates a list of words\n",
    "    words = \"\"\n",
    "    for each_word in tweet:\n",
    "        if each_word not in terms_to_remove:\n",
    "            words = words + \" \" + each_word\n",
    "    # return [word for word in tweet if word not in terms_to_remove]\n",
    "    return words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session, load the dataset into a Spark DataFrame and then adjust column names\n",
    "from pyspark.sql import SparkSession, functions\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Training Twitter Sentiment Analysis\").getOrCreate()\n",
    "training_data = spark.read.load(\n",
    "    \"tmp/training.1600000.processed.noemoticon.csv\",\n",
    "    format=\"csv\")\n",
    "training_data = training_data.withColumnRenamed(\"_c0\", \"label\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"tweet_id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"query\") \\\n",
    "    .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "    .withColumnRenamed(\"_c5\", \"tweet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are loading just a bunch of lines locally. On the server we will use the whole dataset to train the model\n",
    "sample_size = 20000\n",
    "training_data = training_data.sample(sample_size / training_data.count())\n",
    "\n",
    "training_data = training_data.select(functions.col(\"label\"), functions.col(\"tweet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               tweet|      tweet_cleansed|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|spring break in p...|spring break plai...|\n",
      "|    0|i think my arms a...|think arms sore t...|\n",
      "|    0|@SarahReedSC trea...| treaty isnt defined|\n",
      "|    0|Think I'm going t...|think im going be...|\n",
      "|    0|Uh oh... I think ...|uh oh think getti...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the cleansing UDF for tweet column\n",
    "udf_cleansing = functions.udf(cleansing)\n",
    "training_data = training_data.withColumn(\"tweet_cleansed\", udf_cleansing(functions.col(\"tweet\")))\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "The following code snippet creates a list of every remaining word (after cleansing) that will be used to build the features for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|               tweet|      tweet_cleansed|               words|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|spring break in p...|spring break plai...|[spring, break, p...|\n",
      "|    0|i think my arms a...|think arms sore t...|[think, arms, sor...|\n",
      "|    0|@SarahReedSC trea...| treaty isnt defined|[treaty, isnt, de...|\n",
      "|    0|Think I'm going t...|think im going be...|[think, im, going...|\n",
      "|    0|Uh oh... I think ...|uh oh think getti...|[uh, oh, think, g...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_cleansed\", outputCol=\"words\")\n",
    "training_data = tokenizer.transform(training_data)\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF\n",
    "\n",
    "The following code snippet creates the features, which means a list of all terms available on all \"documents\" with a \"tag\" indicating its presence or not on the specific \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               tweet|      tweet_cleansed|               words|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|spring break in p...|spring break plai...|[spring, break, p...|(262144,[36879,12...|\n",
      "|    0|i think my arms a...|think arms sore t...|[think, arms, sor...|(262144,[46044,13...|\n",
      "|    0|@SarahReedSC trea...| treaty isnt defined|[treaty, isnt, de...|(262144,[113957,1...|\n",
      "|    0|Think I'm going t...|think im going be...|[think, im, going...|(262144,[21641,31...|\n",
      "|    0|Uh oh... I think ...|uh oh think getti...|[uh, oh, think, g...|(262144,[18184,15...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "training_data = hashingTF.transform(training_data)\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|               tweet|      tweet_cleansed|               words|            features|labelIndex|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|spring break in p...|spring break plai...|[spring, break, p...|(262144,[36879,12...|       0.0|\n",
      "|    0|i think my arms a...|think arms sore t...|[think, arms, sor...|(262144,[46044,13...|       0.0|\n",
      "|    0|@SarahReedSC trea...| treaty isnt defined|[treaty, isnt, de...|(262144,[113957,1...|       0.0|\n",
      "|    0|Think I'm going t...|think im going be...|[think, im, going...|(262144,[21641,31...|       0.0|\n",
      "|    0|Uh oh... I think ...|uh oh think getti...|[uh, oh, think, g...|(262144,[18184,15...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a numeric index for the labels\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "model = stringIndexer.fit(training_data)\n",
    "training_data = model.transform(training_data)\n",
    "training_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test\n",
    "training, test = training_data.randomSplit([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model\n",
    "In this step we train the NaiveBayes model using our training slice. Then, we use the test slice to evaluate the level of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 10242 \n",
      "Correct: 7393 \n",
      "Accuracy: 0.7218316735012693\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "#Naive bayes\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"labelIndex\", predictionCol=\"NB_pred\",\n",
    "                probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "nbModel = nb.fit(training)\n",
    "cv = nbModel.transform(test)\n",
    "total = cv.count()\n",
    "correct = cv.where(cv['labelIndex'] == cv['NB_pred']).count()\n",
    "accuracy = correct/total\n",
    "\n",
    "print(\n",
    "    \"\\nTotal:\", total, \n",
    "    \"\\nCorrect:\", correct, \n",
    "    \"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+-------+--------------------+--------------------+\n",
      "|label|               tweet|labelIndex|NB_pred|          NB_rawPred|             NB_prob|\n",
      "+-----+--------------------+----------+-------+--------------------+--------------------+\n",
      "|    0|       FS keeps c...|       0.0|    0.0|[-78.703316673940...|[0.79291434174872...|\n",
      "|    0|   (must i say mo...|       0.0|    1.0|[-19.507001476810...|[0.35424702859924...|\n",
      "|    0|  I TALKED TO U B...|       0.0|    0.0|[-56.236574224334...|[0.73819625884837...|\n",
      "|    0|  i was too slow ...|       0.0|    0.0|[-38.401551806386...|[0.63381871955419...|\n",
      "|    0|  im sick  'cough...|       0.0|    0.0|[-36.952716408033...|[0.99598596262325...|\n",
      "|    0| #IMISSCATH #IMIS...|       0.0|    1.0|[-152.00561764964...|[0.47699695038052...|\n",
      "|    0| #p1wimax no sign...|       0.0|    1.0|[-49.748570255830...|[0.49022116629620...|\n",
      "|    0| ....  i don't kn...|       0.0|    0.0|[-15.160465677834...|[0.67468331301980...|\n",
      "|    0| ;( noooo! why? t...|       0.0|    0.0|[-66.408307330115...|[0.90480248480954...|\n",
      "|    0| @lolife4life whe...|       0.0|    0.0|[-51.816528129391...|[0.79473311914753...|\n",
      "|    0| @tweetdeck lost ...|       0.0|    1.0|[-67.582293407541...|[0.41524171650296...|\n",
      "|    0| Climate Progress...|       0.0|    1.0|[-175.14386436613...|[0.03614611469135...|\n",
      "|    0| I hate not havin...|       0.0|    0.0|[-27.602170517980...|[0.91049287797967...|\n",
      "|    0| I just want to s...|       0.0|    0.0|[-16.164802435910...|[0.79052339067859...|\n",
      "|    0| I wish I wasn't ...|       0.0|    0.0|[-41.084821219476...|[0.94616345188444...|\n",
      "|    0| Im in trouble fo...|       0.0|    0.0|[-45.925337535173...|[0.92994300124875...|\n",
      "|    0| MCHAMMER touched...|       0.0|    0.0|[-54.471332983141...|[0.77000504482622...|\n",
      "|    0| My pic isn't rig...|       0.0|    0.0|[-112.75835190163...|[0.85110336245167...|\n",
      "|    0| My turtle food! ...|       0.0|    0.0|[-94.006155092209...|[0.91739452516998...|\n",
      "|    0| Playoffs it is then|       0.0|    0.0|[-12.615185049129...|[0.66237268236971...|\n",
      "+-----+--------------------+----------+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv.select(\"label\", \"tweet\", \"labelIndex\", \"NB_pred\", \"NB_rawPred\", \"NB_prob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving trained model for usage in a Pipeline (so you don't need to re-train everytime you need to use it)\n",
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    \n",
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment_spark\")\n",
    "nbModel.write().overwrite().save(model_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary folder\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

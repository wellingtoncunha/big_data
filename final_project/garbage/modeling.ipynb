{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_folder = os.getcwd()\n",
    "training_dataset_path = os.path.join(\n",
    "    base_folder, \n",
    "    \"trainingandtestdata\", \n",
    "    \"training.1600000.processed.noemoticon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(\n",
    "    training_dataset_path, \n",
    "    encoding=\"latin-1\", \n",
    "    warn_bad_lines=True,\n",
    "    error_bad_lines=False,\n",
    "    header=None, \n",
    "    names=[\"polarity\", \"id\", \"date\", \"query\", \"user\", \"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1275757</th>\n",
       "      <td>4</td>\n",
       "      <td>2001124104</td>\n",
       "      <td>Mon Jun 01 23:54:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lucytaylor123</td>\n",
       "      <td>waiting for lulu, while i wait im looking on y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10947</th>\n",
       "      <td>0</td>\n",
       "      <td>1551136705</td>\n",
       "      <td>Sat Apr 18 08:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sdohana</td>\n",
       "      <td>bummed we're not in puerto rico rappelling dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466283</th>\n",
       "      <td>0</td>\n",
       "      <td>2175529740</td>\n",
       "      <td>Mon Jun 15 01:14:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>triiishh</td>\n",
       "      <td>@eripeng Hahaha. To tweet is hard hard work.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350408</th>\n",
       "      <td>0</td>\n",
       "      <td>2017828702</td>\n",
       "      <td>Wed Jun 03 09:00:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>louisewaves</td>\n",
       "      <td>@takebrokenme  Poor u!  U had enough fluids &amp;a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722368</th>\n",
       "      <td>0</td>\n",
       "      <td>2261462524</td>\n",
       "      <td>Sat Jun 20 20:49:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>felecia91</td>\n",
       "      <td>is super frustrated!!! *-* Needs God's help de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539685</th>\n",
       "      <td>4</td>\n",
       "      <td>2180202998</td>\n",
       "      <td>Mon Jun 15 10:08:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheAngelPandora</td>\n",
       "      <td>Wishing some of my friends had twitter so I ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394557</th>\n",
       "      <td>0</td>\n",
       "      <td>2055662923</td>\n",
       "      <td>Sat Jun 06 09:43:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sweet0nes</td>\n",
       "      <td>Locked my keys in the trunk@walmart! We r wait...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211470</th>\n",
       "      <td>4</td>\n",
       "      <td>1989155805</td>\n",
       "      <td>Mon Jun 01 00:31:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElsaEnah</td>\n",
       "      <td>just got homeIsawI love you man ... very funny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976352</th>\n",
       "      <td>4</td>\n",
       "      <td>1833625317</td>\n",
       "      <td>Sun May 17 23:54:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cdub4</td>\n",
       "      <td>@DwightHoward congrats!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309938</th>\n",
       "      <td>4</td>\n",
       "      <td>2013178258</td>\n",
       "      <td>Tue Jun 02 22:05:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AymeeBridesFL</td>\n",
       "      <td>Just finished the 98 name badges for the bride...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity          id                          date     query  \\\n",
       "1275757         4  2001124104  Mon Jun 01 23:54:51 PDT 2009  NO_QUERY   \n",
       "10947           0  1551136705  Sat Apr 18 08:14:47 PDT 2009  NO_QUERY   \n",
       "466283          0  2175529740  Mon Jun 15 01:14:45 PDT 2009  NO_QUERY   \n",
       "350408          0  2017828702  Wed Jun 03 09:00:54 PDT 2009  NO_QUERY   \n",
       "722368          0  2261462524  Sat Jun 20 20:49:47 PDT 2009  NO_QUERY   \n",
       "...           ...         ...                           ...       ...   \n",
       "1539685         4  2180202998  Mon Jun 15 10:08:56 PDT 2009  NO_QUERY   \n",
       "394557          0  2055662923  Sat Jun 06 09:43:45 PDT 2009  NO_QUERY   \n",
       "1211470         4  1989155805  Mon Jun 01 00:31:56 PDT 2009  NO_QUERY   \n",
       "976352          4  1833625317  Sun May 17 23:54:15 PDT 2009  NO_QUERY   \n",
       "1309938         4  2013178258  Tue Jun 02 22:05:01 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \n",
       "1275757    lucytaylor123  waiting for lulu, while i wait im looking on y...  \n",
       "10947            sdohana  bummed we're not in puerto rico rappelling dow...  \n",
       "466283          triiishh      @eripeng Hahaha. To tweet is hard hard work.   \n",
       "350408       louisewaves  @takebrokenme  Poor u!  U had enough fluids &a...  \n",
       "722368         felecia91  is super frustrated!!! *-* Needs God's help de...  \n",
       "...                  ...                                                ...  \n",
       "1539685  TheAngelPandora  Wishing some of my friends had twitter so I ca...  \n",
       "394557         sweet0nes  Locked my keys in the trunk@walmart! We r wait...  \n",
       "1211470         ElsaEnah  just got homeIsawI love you man ... very funny...  \n",
       "976352             cdub4                           @DwightHoward congrats!   \n",
       "1309938    AymeeBridesFL  Just finished the 98 name badges for the bride...  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = training_dataset.sample(10000)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5035\n",
       "4    4965\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleansing text\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet).get_text() # extract text from HTML\n",
    "    tweet = tweet.lower() # convert text to lower-case\n",
    "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) # remove URLs\n",
    "    tweet = re.sub('@[^\\s]+', '', tweet) # remove usernames\n",
    "    tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "    tweet = word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)    \n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "    tweet = [word for word in tweet if word not in stopwords_en] #remove stop words\n",
    "    return ' '.join(tweet)\n",
    "\n",
    "training_dataset[\"cleansed\"] = training_dataset.apply(lambda row : clean_tweet(row[\"tweet\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>cleansed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1275757</th>\n",
       "      <td>4</td>\n",
       "      <td>2001124104</td>\n",
       "      <td>Mon Jun 01 23:54:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lucytaylor123</td>\n",
       "      <td>waiting for lulu, while i wait im looking on y...</td>\n",
       "      <td>waiting lulu , wait im looking youtube cool st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10947</th>\n",
       "      <td>0</td>\n",
       "      <td>1551136705</td>\n",
       "      <td>Sat Apr 18 08:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sdohana</td>\n",
       "      <td>bummed we're not in puerto rico rappelling dow...</td>\n",
       "      <td>bummed 're puerto rico rappelling waterfall , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466283</th>\n",
       "      <td>0</td>\n",
       "      <td>2175529740</td>\n",
       "      <td>Mon Jun 15 01:14:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>triiishh</td>\n",
       "      <td>@eripeng Hahaha. To tweet is hard hard work.</td>\n",
       "      <td>hahaha . tweet hard hard work .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350408</th>\n",
       "      <td>0</td>\n",
       "      <td>2017828702</td>\n",
       "      <td>Wed Jun 03 09:00:54 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>louisewaves</td>\n",
       "      <td>@takebrokenme  Poor u!  U had enough fluids &amp;a...</td>\n",
       "      <td>poor u ! u enough fluids &amp; salts ? want leave ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722368</th>\n",
       "      <td>0</td>\n",
       "      <td>2261462524</td>\n",
       "      <td>Sat Jun 20 20:49:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>felecia91</td>\n",
       "      <td>is super frustrated!!! *-* Needs God's help de...</td>\n",
       "      <td>super frustrated ! ! ! * - * needs god 's help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539685</th>\n",
       "      <td>4</td>\n",
       "      <td>2180202998</td>\n",
       "      <td>Mon Jun 15 10:08:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheAngelPandora</td>\n",
       "      <td>Wishing some of my friends had twitter so I ca...</td>\n",
       "      <td>wishing friends twitter bug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394557</th>\n",
       "      <td>0</td>\n",
       "      <td>2055662923</td>\n",
       "      <td>Sat Jun 06 09:43:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sweet0nes</td>\n",
       "      <td>Locked my keys in the trunk@walmart! We r wait...</td>\n",
       "      <td>locked keys trunk r waiting 4 locksmith . $ 55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211470</th>\n",
       "      <td>4</td>\n",
       "      <td>1989155805</td>\n",
       "      <td>Mon Jun 01 00:31:56 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElsaEnah</td>\n",
       "      <td>just got homeIsawI love you man ... very funny...</td>\n",
       "      <td>got homeisawi love man ... funny movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976352</th>\n",
       "      <td>4</td>\n",
       "      <td>1833625317</td>\n",
       "      <td>Sun May 17 23:54:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cdub4</td>\n",
       "      <td>@DwightHoward congrats!</td>\n",
       "      <td>congrats !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309938</th>\n",
       "      <td>4</td>\n",
       "      <td>2013178258</td>\n",
       "      <td>Tue Jun 02 22:05:01 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AymeeBridesFL</td>\n",
       "      <td>Just finished the 98 name badges for the bride...</td>\n",
       "      <td>finished 98 name badges brides networking tomo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         polarity          id                          date     query  \\\n",
       "1275757         4  2001124104  Mon Jun 01 23:54:51 PDT 2009  NO_QUERY   \n",
       "10947           0  1551136705  Sat Apr 18 08:14:47 PDT 2009  NO_QUERY   \n",
       "466283          0  2175529740  Mon Jun 15 01:14:45 PDT 2009  NO_QUERY   \n",
       "350408          0  2017828702  Wed Jun 03 09:00:54 PDT 2009  NO_QUERY   \n",
       "722368          0  2261462524  Sat Jun 20 20:49:47 PDT 2009  NO_QUERY   \n",
       "...           ...         ...                           ...       ...   \n",
       "1539685         4  2180202998  Mon Jun 15 10:08:56 PDT 2009  NO_QUERY   \n",
       "394557          0  2055662923  Sat Jun 06 09:43:45 PDT 2009  NO_QUERY   \n",
       "1211470         4  1989155805  Mon Jun 01 00:31:56 PDT 2009  NO_QUERY   \n",
       "976352          4  1833625317  Sun May 17 23:54:15 PDT 2009  NO_QUERY   \n",
       "1309938         4  2013178258  Tue Jun 02 22:05:01 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                              tweet  \\\n",
       "1275757    lucytaylor123  waiting for lulu, while i wait im looking on y...   \n",
       "10947            sdohana  bummed we're not in puerto rico rappelling dow...   \n",
       "466283          triiishh      @eripeng Hahaha. To tweet is hard hard work.    \n",
       "350408       louisewaves  @takebrokenme  Poor u!  U had enough fluids &a...   \n",
       "722368         felecia91  is super frustrated!!! *-* Needs God's help de...   \n",
       "...                  ...                                                ...   \n",
       "1539685  TheAngelPandora  Wishing some of my friends had twitter so I ca...   \n",
       "394557         sweet0nes  Locked my keys in the trunk@walmart! We r wait...   \n",
       "1211470         ElsaEnah  just got homeIsawI love you man ... very funny...   \n",
       "976352             cdub4                           @DwightHoward congrats!    \n",
       "1309938    AymeeBridesFL  Just finished the 98 name badges for the bride...   \n",
       "\n",
       "                                                  cleansed  \n",
       "1275757  waiting lulu , wait im looking youtube cool st...  \n",
       "10947    bummed 're puerto rico rappelling waterfall , ...  \n",
       "466283                     hahaha . tweet hard hard work .  \n",
       "350408   poor u ! u enough fluids & salts ? want leave ...  \n",
       "722368   super frustrated ! ! ! * - * needs god 's help...  \n",
       "...                                                    ...  \n",
       "1539685                        wishing friends twitter bug  \n",
       "394557   locked keys trunk r waiting 4 locksmith . $ 55...  \n",
       "1211470             got homeisawi love man ... funny movie  \n",
       "976352                                          congrats !  \n",
       "1309938  finished 98 name badges brides networking tomo...  \n",
       "\n",
       "[10000 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = training_dataset.iloc[:, 6].values\n",
    "labels = training_dataset.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['waiting lulu , wait im looking youtube cool stuff ummmmm',\n",
       "       \"bummed 're puerto rico rappelling waterfall , zip lining bungee jumping couple boo coming back early .\",\n",
       "       'hahaha . tweet hard hard work .', ...,\n",
       "       'got homeisawi love man ... funny movie', 'congrats !',\n",
       "       'finished 98 name badges brides networking tomorrow ! see everyone'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500) #, min_df=7, max_df=0.8)\n",
    "vectorized_features = vectorizer.fit_transform(features).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '09',\n",
       " '10',\n",
       " '100',\n",
       " '10th',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '140',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2009',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '2day',\n",
       " '2morrow',\n",
       " '2nd',\n",
       " '30',\n",
       " '31',\n",
       " '33',\n",
       " '360',\n",
       " '3am',\n",
       " '3d',\n",
       " '3g',\n",
       " '3rd',\n",
       " '40',\n",
       " '45',\n",
       " '4th',\n",
       " '50',\n",
       " '5am',\n",
       " '5th',\n",
       " '60',\n",
       " '7pm',\n",
       " '80',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'abt',\n",
       " 'ac',\n",
       " 'accident',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accounts',\n",
       " 'ace',\n",
       " 'ache',\n",
       " 'across',\n",
       " 'act',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actor',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'admit',\n",
       " 'adorable',\n",
       " 'ads',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'afraid',\n",
       " 'afternoon',\n",
       " 'ages',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'ah',\n",
       " 'aha',\n",
       " 'ahaha',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhh',\n",
       " 'ai',\n",
       " 'aim',\n",
       " 'aint',\n",
       " 'air',\n",
       " 'airport',\n",
       " 'album',\n",
       " 'alcohol',\n",
       " 'alex',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'allergic',\n",
       " 'allowed',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alot',\n",
       " 'already',\n",
       " 'alright',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amanda',\n",
       " 'amazing',\n",
       " 'amber',\n",
       " 'amen',\n",
       " 'america',\n",
       " 'american',\n",
       " 'amy',\n",
       " 'and',\n",
       " 'andy',\n",
       " 'angels',\n",
       " 'angry',\n",
       " 'animal',\n",
       " 'ankle',\n",
       " 'anniversary',\n",
       " 'annoy',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'apple',\n",
       " 'application',\n",
       " 'appreciate',\n",
       " 'apps',\n",
       " 'area',\n",
       " 'arent',\n",
       " 'argentina',\n",
       " 'argh',\n",
       " 'arm',\n",
       " 'around',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'article',\n",
       " 'asap',\n",
       " 'ashley',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asleep',\n",
       " 'ass',\n",
       " 'assignment',\n",
       " 'assignments',\n",
       " 'aswell',\n",
       " 'ate',\n",
       " 'atl',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'attend',\n",
       " 'attention',\n",
       " 'august',\n",
       " 'australia',\n",
       " 'available',\n",
       " 'avatar',\n",
       " 'avoid',\n",
       " 'aw',\n",
       " 'awake',\n",
       " 'awards',\n",
       " 'away',\n",
       " 'awe',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'aww',\n",
       " 'awww',\n",
       " 'awwww',\n",
       " 'aye',\n",
       " 'b4',\n",
       " 'babe',\n",
       " 'babes',\n",
       " 'babies',\n",
       " 'baby',\n",
       " 'babysitting',\n",
       " 'back',\n",
       " 'background',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'bah',\n",
       " 'baking',\n",
       " 'balls',\n",
       " 'band',\n",
       " 'bangs',\n",
       " 'bank',\n",
       " 'bar',\n",
       " 'barely',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'basically',\n",
       " 'basis',\n",
       " 'basketball',\n",
       " 'bath',\n",
       " 'bathroom',\n",
       " 'battery',\n",
       " 'bay',\n",
       " 'bb',\n",
       " 'bbc',\n",
       " 'bbq',\n",
       " 'bc',\n",
       " 'bday',\n",
       " 'beach',\n",
       " 'beans',\n",
       " 'bear',\n",
       " 'beat',\n",
       " 'beats',\n",
       " 'beautiful',\n",
       " 'beauty',\n",
       " 'become',\n",
       " 'becoming',\n",
       " 'becuz',\n",
       " 'bed',\n",
       " 'bedtime',\n",
       " 'beer',\n",
       " 'begins',\n",
       " 'behind',\n",
       " 'bein',\n",
       " 'believe',\n",
       " 'belly',\n",
       " 'ben',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'beta',\n",
       " 'better',\n",
       " 'beyond',\n",
       " 'bf',\n",
       " 'bff',\n",
       " 'bgt',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bike',\n",
       " 'bill',\n",
       " 'billy',\n",
       " 'bing',\n",
       " 'bio',\n",
       " 'biology',\n",
       " 'bird',\n",
       " 'birds',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bitch',\n",
       " 'bite',\n",
       " 'bk',\n",
       " 'black',\n",
       " 'blackberry',\n",
       " 'blame',\n",
       " 'blanket',\n",
       " 'blast',\n",
       " 'bleeding',\n",
       " 'bless',\n",
       " 'blessed',\n",
       " 'blew',\n",
       " 'bliss',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blog',\n",
       " 'blonde',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blown',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'bn',\n",
       " 'board',\n",
       " 'boat',\n",
       " 'bob',\n",
       " 'body',\n",
       " 'bold',\n",
       " 'bomb',\n",
       " 'boo',\n",
       " 'book',\n",
       " 'booked',\n",
       " 'books',\n",
       " 'boot',\n",
       " 'bored',\n",
       " 'boredom',\n",
       " 'boring',\n",
       " 'born',\n",
       " 'borrow',\n",
       " 'boss',\n",
       " 'boston',\n",
       " 'bothered',\n",
       " 'bottle',\n",
       " 'bought',\n",
       " 'bout',\n",
       " 'bowl',\n",
       " 'bowling',\n",
       " 'box',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'boys',\n",
       " 'brain',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'breaking',\n",
       " 'bright',\n",
       " 'brilliant',\n",
       " 'bring',\n",
       " 'bro',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'bros',\n",
       " 'brother',\n",
       " 'brothers',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bt',\n",
       " 'btw',\n",
       " 'bubble',\n",
       " 'bucks',\n",
       " 'buddy',\n",
       " 'bug',\n",
       " 'bull',\n",
       " 'bum',\n",
       " 'bummed',\n",
       " 'bummer',\n",
       " 'bunch',\n",
       " 'burgers',\n",
       " 'burn',\n",
       " 'burning',\n",
       " 'burns',\n",
       " 'burnt',\n",
       " 'bus',\n",
       " 'business',\n",
       " 'busy',\n",
       " 'butt',\n",
       " 'butter',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'bye',\n",
       " 'ca',\n",
       " 'cab',\n",
       " 'cable',\n",
       " 'cafe',\n",
       " 'cake',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'calling',\n",
       " 'calls',\n",
       " 'came',\n",
       " 'camera',\n",
       " 'camp',\n",
       " 'canada',\n",
       " 'canceled',\n",
       " 'cancelled',\n",
       " 'candy',\n",
       " 'cant',\n",
       " 'car',\n",
       " 'card',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'careful',\n",
       " 'cars',\n",
       " 'case',\n",
       " 'cash',\n",
       " 'cast',\n",
       " 'cat',\n",
       " 'catch',\n",
       " 'catching',\n",
       " 'cats',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'cavs',\n",
       " 'cd',\n",
       " 'cell',\n",
       " 'center',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chair',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'chat',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'checking',\n",
       " 'cheer',\n",
       " 'cheers',\n",
       " 'cheese',\n",
       " 'chelsea',\n",
       " 'cherries',\n",
       " 'chicago',\n",
       " 'chick',\n",
       " 'chicken',\n",
       " 'child',\n",
       " 'children',\n",
       " 'chili',\n",
       " 'chill',\n",
       " 'chillin',\n",
       " 'chilling',\n",
       " 'chinese',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chris',\n",
       " 'chuck',\n",
       " 'chunk',\n",
       " 'church',\n",
       " 'cinema',\n",
       " 'city',\n",
       " 'class',\n",
       " 'classes',\n",
       " 'clean',\n",
       " 'cleaned',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clever',\n",
       " 'clicked',\n",
       " 'client',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closer',\n",
       " 'closet',\n",
       " 'closing',\n",
       " 'clothes',\n",
       " 'clouds',\n",
       " 'cloudy',\n",
       " 'club',\n",
       " 'co',\n",
       " 'coach',\n",
       " 'coast',\n",
       " 'code',\n",
       " 'coffee',\n",
       " 'coke',\n",
       " 'cold',\n",
       " 'college',\n",
       " 'color',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comedy',\n",
       " 'comes',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'comments',\n",
       " 'community',\n",
       " 'company',\n",
       " 'competition',\n",
       " 'complete',\n",
       " 'completed',\n",
       " 'completely',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'conan',\n",
       " 'concentrate',\n",
       " 'concert',\n",
       " 'conference',\n",
       " 'confused',\n",
       " 'confusing',\n",
       " 'congrats',\n",
       " 'congratulations',\n",
       " 'connection',\n",
       " 'considering',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'contest',\n",
       " 'control',\n",
       " 'conversation',\n",
       " 'convince',\n",
       " 'cook',\n",
       " 'cookie',\n",
       " 'cookies',\n",
       " 'cooking',\n",
       " 'cool',\n",
       " 'coolest',\n",
       " 'copy',\n",
       " 'corn',\n",
       " 'corner',\n",
       " 'cos',\n",
       " 'cost',\n",
       " 'couch',\n",
       " 'cough',\n",
       " 'coughing',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'count',\n",
       " 'counter',\n",
       " 'counting',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'court',\n",
       " 'cousin',\n",
       " 'cousins',\n",
       " 'cover',\n",
       " 'coz',\n",
       " 'crap',\n",
       " 'crappy',\n",
       " 'crashed',\n",
       " 'crashing',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'creating',\n",
       " 'creative',\n",
       " 'credit',\n",
       " 'creepy',\n",
       " 'crew',\n",
       " 'cried',\n",
       " 'cross',\n",
       " 'crossed',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'crystal',\n",
       " 'cuddle',\n",
       " 'cup',\n",
       " 'cupcake',\n",
       " 'currently',\n",
       " 'cus',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cutie',\n",
       " 'cuz',\n",
       " 'cyrus',\n",
       " 'da',\n",
       " 'dad',\n",
       " 'daddy',\n",
       " 'dads',\n",
       " 'daily',\n",
       " 'dam',\n",
       " 'dammit',\n",
       " 'damn',\n",
       " 'damnit',\n",
       " 'dance',\n",
       " 'dancing',\n",
       " 'dang',\n",
       " 'danny',\n",
       " 'dark',\n",
       " 'darling',\n",
       " 'darn',\n",
       " 'dat',\n",
       " 'date',\n",
       " 'dates',\n",
       " 'daughter',\n",
       " 'dave',\n",
       " 'david',\n",
       " 'davis',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dc',\n",
       " 'de',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'dealing',\n",
       " 'dear',\n",
       " 'death',\n",
       " 'debate',\n",
       " 'debut',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'def',\n",
       " 'definately',\n",
       " 'definitely',\n",
       " 'degrees',\n",
       " 'delay',\n",
       " 'delete',\n",
       " 'delicious',\n",
       " 'delivery',\n",
       " 'demi',\n",
       " 'demons',\n",
       " 'dentist',\n",
       " 'denver',\n",
       " 'depends',\n",
       " 'depressed',\n",
       " 'deserve',\n",
       " 'design',\n",
       " 'desk',\n",
       " 'desperate',\n",
       " 'destroying',\n",
       " 'details',\n",
       " 'devon',\n",
       " 'didnt',\n",
       " 'die',\n",
       " 'died',\n",
       " 'dies',\n",
       " 'diet',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'diky',\n",
       " 'din',\n",
       " 'dinner',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'dirty',\n",
       " 'disappointed',\n",
       " 'disney',\n",
       " 'distracted',\n",
       " 'diversity',\n",
       " 'dj',\n",
       " 'dl',\n",
       " 'dm',\n",
       " 'dnt',\n",
       " 'doc',\n",
       " 'doctor',\n",
       " 'doesnt',\n",
       " 'dog',\n",
       " 'doggy',\n",
       " 'dogs',\n",
       " 'doin',\n",
       " 'dolls',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'door',\n",
       " 'dork',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'download',\n",
       " 'downtown',\n",
       " 'dp',\n",
       " 'dr',\n",
       " 'drag',\n",
       " 'drama',\n",
       " 'drank',\n",
       " 'draw',\n",
       " 'dream',\n",
       " 'dreaming',\n",
       " 'dreams',\n",
       " 'dress',\n",
       " 'dressed',\n",
       " 'drew',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'drinks',\n",
       " 'drive',\n",
       " 'driver',\n",
       " 'drivin',\n",
       " 'driving',\n",
       " 'drop',\n",
       " 'dropped',\n",
       " 'dropping',\n",
       " 'drunk',\n",
       " 'dry',\n",
       " 'dude',\n",
       " 'due',\n",
       " 'dull',\n",
       " 'dumb',\n",
       " 'dunno',\n",
       " 'dvd',\n",
       " 'dye',\n",
       " 'dying',\n",
       " 'e3',\n",
       " 'ear',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'earth',\n",
       " 'easier',\n",
       " 'east',\n",
       " 'easter',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eating',\n",
       " 'ebay',\n",
       " 'ed',\n",
       " 'editing',\n",
       " 'education',\n",
       " 'edward',\n",
       " 'effing',\n",
       " 'egg',\n",
       " 'eh',\n",
       " 'either',\n",
       " 'el',\n",
       " 'else',\n",
       " 'em',\n",
       " 'email',\n",
       " 'emails',\n",
       " 'emo',\n",
       " 'empty',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'ending',\n",
       " 'ends',\n",
       " 'energy',\n",
       " 'england',\n",
       " 'english',\n",
       " 'enjoy',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'epic',\n",
       " 'episode',\n",
       " 'episodes',\n",
       " 'eric',\n",
       " 'esp',\n",
       " 'especially',\n",
       " 'espresso',\n",
       " 'essay',\n",
       " 'est',\n",
       " 'etc',\n",
       " 'eurovision',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everyones',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'evil',\n",
       " 'ew',\n",
       " 'exactly',\n",
       " 'exam',\n",
       " 'exams',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'excited',\n",
       " 'exciting',\n",
       " 'excuse',\n",
       " 'exhausted',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'explain',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyed',\n",
       " 'eyes',\n",
       " 'f1',\n",
       " 'fa',\n",
       " 'fab',\n",
       " 'fabulous',\n",
       " 'face',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fail',\n",
       " 'fair',\n",
       " 'faith',\n",
       " 'fake',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'falls',\n",
       " 'false',\n",
       " 'fam',\n",
       " 'family',\n",
       " 'famous',\n",
       " 'fan',\n",
       " 'fancy',\n",
       " 'fans',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'farm',\n",
       " 'farrah',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'faster',\n",
       " 'fat',\n",
       " 'father',\n",
       " 'fathers',\n",
       " 'fault',\n",
       " 'fav',\n",
       " 'fave',\n",
       " 'favorite',\n",
       " 'favourite',\n",
       " 'fawcett',\n",
       " 'fb',\n",
       " 'fear',\n",
       " 'feature',\n",
       " 'fed',\n",
       " 'feed',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'feelin',\n",
       " 'feeling',\n",
       " 'feelings',\n",
       " 'feels',\n",
       " 'feet',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'festival',\n",
       " 'fever',\n",
       " 'ff',\n",
       " 'fight',\n",
       " 'fighting',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'figures',\n",
       " 'files',\n",
       " 'filled',\n",
       " 'film',\n",
       " 'final',\n",
       " 'finale',\n",
       " 'finally',\n",
       " 'finals',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'fine',\n",
       " 'finger',\n",
       " 'fingers',\n",
       " 'finish',\n",
       " 'finished',\n",
       " 'finishing',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'fish',\n",
       " 'fit',\n",
       " 'fitness',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'fixed',\n",
       " 'fixing',\n",
       " 'flat',\n",
       " 'fletcher',\n",
       " 'flickr',\n",
       " 'flight',\n",
       " 'floor',\n",
       " 'flu',\n",
       " 'fly',\n",
       " 'flying',\n",
       " 'fm',\n",
       " 'fml',\n",
       " 'folks',\n",
       " 'follow',\n",
       " 'follower',\n",
       " 'followers',\n",
       " 'followfriday',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'food',\n",
       " 'foot',\n",
       " 'football',\n",
       " 'forever',\n",
       " 'forget',\n",
       " 'forgive',\n",
       " 'forgot',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'formula',\n",
       " 'forums',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'fox',\n",
       " 'fr',\n",
       " 'france',\n",
       " 'freakin',\n",
       " 'freaking',\n",
       " 'free',\n",
       " 'freedom',\n",
       " 'freezing',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'fri',\n",
       " 'friday',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friends',\n",
       " 'frm',\n",
       " 'front',\n",
       " 'fruit',\n",
       " 'frustrated',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'funeral',\n",
       " 'funny',\n",
       " 'fur',\n",
       " 'future',\n",
       " 'gah',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gap',\n",
       " 'garage',\n",
       " 'garden',\n",
       " 'gas',\n",
       " 'gave',\n",
       " 'gay',\n",
       " 'geez',\n",
       " 'george',\n",
       " 'german',\n",
       " 'germany',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'gettin',\n",
       " 'getting',\n",
       " 'gf',\n",
       " 'ghost',\n",
       " 'giant',\n",
       " 'gift',\n",
       " 'gig',\n",
       " 'girl',\n",
       " 'girlfriend',\n",
       " 'girlie',\n",
       " 'girls',\n",
       " 'girly',\n",
       " 'give',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glass',\n",
       " 'glasses',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'god',\n",
       " 'goes',\n",
       " 'goin',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'golf',\n",
       " 'gon',\n",
       " 'gona',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'goodbye',\n",
       " 'goodmorning',\n",
       " 'goodness',\n",
       " 'goodnight',\n",
       " 'goodnite',\n",
       " 'google',\n",
       " 'gorgeous',\n",
       " 'gosh',\n",
       " 'gossip',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'gr8',\n",
       " 'grab',\n",
       " 'grad',\n",
       " 'grade',\n",
       " 'graduation',\n",
       " 'grand',\n",
       " 'grandma',\n",
       " 'grandpa',\n",
       " 'grandparents',\n",
       " 'grass',\n",
       " 'great',\n",
       " 'greatest',\n",
       " 'green',\n",
       " 'greet',\n",
       " 'grey',\n",
       " 'gross',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'grown',\n",
       " 'grr',\n",
       " 'grrr',\n",
       " 'grumpy',\n",
       " 'gt',\n",
       " 'guess',\n",
       " 'guest',\n",
       " 'guilt',\n",
       " 'guilty',\n",
       " 'guitar',\n",
       " 'gunna',\n",
       " 'gutted',\n",
       " 'guy',\n",
       " 'guys',\n",
       " 'gym',\n",
       " 'ha',\n",
       " 'haa',\n",
       " 'hah',\n",
       " 'haha',\n",
       " 'hahah',\n",
       " 'hahaha',\n",
       " 'hahahaha',\n",
       " 'hair',\n",
       " 'haircut',\n",
       " 'half',\n",
       " 'halfway',\n",
       " 'halo',\n",
       " 'ham',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hands',\n",
       " 'handsome',\n",
       " 'hang',\n",
       " 'hangin',\n",
       " 'hanging',\n",
       " 'hangover',\n",
       " 'hannah',\n",
       " 'happen',\n",
       " 'happened',\n",
       " ...]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_features, labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, random_state=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[723 275]\n",
      " [286 716]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72       998\n",
      "           4       0.72      0.71      0.72      1002\n",
      "\n",
      "    accuracy                           0.72      2000\n",
      "   macro avg       0.72      0.72      0.72      2000\n",
      "weighted avg       0.72      0.72      0.72      2000\n",
      "\n",
      "0.7195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 0, ..., 0, 0, 4])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_classifier.classify(\"'noel says holla may somethin ya .. & yes shall b pickin ur ass atl airport t-minus 24 days !'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweet):\n",
    "\n",
    "    prediction = text_classifier.predict(tweet)\n",
    "    #print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'classify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-a5f627ddae24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0meach_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tweet\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meach_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vectorized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0meach_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_tweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vectorized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#     print(features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'classify'"
     ]
    }
   ],
   "source": [
    "li = []\n",
    "for index, each_tweet in training_dataset.iterrows():\n",
    "    each_tweet[\"cleansed\"] = clean_tweet(each_tweet[\"tweet\"])\n",
    "    features = [each_tweet[\"tweet\"]]\n",
    "    each_tweet[\"vectorized\"] = vectorizer.fit_transform(features)\n",
    "    each_tweet[\"prediction\"] = text_classifier.classify(each_tweet[\"vectorized\"])\n",
    "    li.append(each_tweet)\n",
    "#     print(features)\n",
    "    \n",
    "new_dataset = pd.DataFrame(li) \n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 53223 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(training_dataset[\"vectorized\"])[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_path = os.path.join(\n",
    "    base_folder, \n",
    "    \"trainingandtestdata\", \n",
    "    \"testdata.manual.2009.06.14.csv\"    \n",
    ")\n",
    "\n",
    "test_dataset = pd.read_csv(\n",
    "    test_dataset_path, \n",
    "    encoding=\"latin-1\", \n",
    "    warn_bad_lines=True,\n",
    "    error_bad_lines=False,\n",
    "    header=None, \n",
    "    names=[\"polarity\", \"id\", \"date\", \"query\", \"user\", \"tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     polarity     id                          date    query             user  \\\n",
       "0           4      3  Mon May 11 03:17:40 UTC 2009  kindle2           tpryan   \n",
       "1           4      4  Mon May 11 03:18:03 UTC 2009  kindle2           vcu451   \n",
       "2           4      5  Mon May 11 03:18:54 UTC 2009  kindle2           chadfu   \n",
       "3           4      6  Mon May 11 03:19:04 UTC 2009  kindle2            SIX15   \n",
       "4           4      7  Mon May 11 03:21:41 UTC 2009  kindle2         yamarama   \n",
       "..        ...    ...                           ...      ...              ...   \n",
       "493         2  14072  Sun Jun 14 04:31:43 UTC 2009    latex          proggit   \n",
       "494         0  14073  Sun Jun 14 04:32:17 UTC 2009    latex           sam33r   \n",
       "495         4  14074  Sun Jun 14 04:36:34 UTC 2009    latex  iamtheonlyjosie   \n",
       "496         0  14075  Sun Jun 14 21:36:07 UTC 2009     iran        plutopup7   \n",
       "497         0  14076  Sun Jun 14 21:36:17 UTC 2009     iran     captain_pete   \n",
       "\n",
       "                                                 tweet  \n",
       "0    @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1    Reading my kindle2...  Love it... Lee childs i...  \n",
       "2    Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3    @kenburbary You'll love your Kindle2. I've had...  \n",
       "4    @mikefish  Fair enough. But i have the Kindle2...  \n",
       "..                                                 ...  \n",
       "493  Ask Programming: LaTeX or InDesign?: submitted...  \n",
       "494  On that note, I hate Word. I hate Pages. I hat...  \n",
       "495  Ahhh... back in a *real* text editing environm...  \n",
       "496  Trouble in Iran, I see. Hmm. Iran. Iran so far...  \n",
       "497  Reading the tweets coming out of Iran... The w...  \n",
       "\n",
       "[498 rows x 6 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

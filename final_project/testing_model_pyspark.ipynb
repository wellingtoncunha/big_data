{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.getcwd()\n",
    "temporary_folder = os.path.join(os.getcwd(), \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files():\n",
    "# Unzip file on a temporary folder\n",
    "    if os.path.exists(temporary_folder):\n",
    "        shutil.rmtree(temporary_folder)\n",
    "        \n",
    "    if not os.path.exists(temporary_folder):\n",
    "        os.makedirs(temporary_folder)\n",
    "        \n",
    "    local_file_name = os.path.join(base_folder, \"training_dataset\", \"trainingandtestdata.zip\")\n",
    "    with zipfile.ZipFile(local_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temporary_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweets\n",
    "\n",
    "The following function prepares the tweet by:\n",
    "\n",
    "* Extracting the text from HTML (for the training dataset provided, we already have the text, but we want to avoid using any HTML tag for classification\n",
    "* Converting all words to lower case\n",
    "* Replacing any URL with \"URL\" constant (to enable the removal of them on a further step)\n",
    "* Replacing any tagging of users with \"USERTAGGING\" (to enable the removal of them in a further step)\n",
    "* Removing any \"#\" from hashtags\n",
    "* Removing punctuation (has little or no weight on classification as it can be used for both intentions)\n",
    "* And finally, removing words and punctuation that has little or no weight on classification (and can even create biases):\n",
    "    * Stop words: set of common words that are used doesn't matter the intenttion (things like it, that, a, the)\n",
    "    * Remove the two constants that we used to replace user tagging and URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(tweet):\n",
    "# Cleansing tweet\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords \n",
    "    from string import punctuation \n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    terms_to_remove = set(stopwords.words(\"english\") + [\"USERTAGGING\",\"URL\"])\n",
    "    tweet = BeautifulSoup(tweet, 'html.parser').get_text() # Extracts text from HTML (just in case!)\n",
    "    tweet = tweet.lower() # Converts text to lower-case\n",
    "    tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet) # Replces URLs by URL constan\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"USERTAGGING\", tweet) # Replaces usernames by USERTAGGING constant \n",
    "    tweet = re.sub(r\"#([^\\s]+)\", r\"\\1\", tweet) # Removes the # in #hashtag\n",
    "    for p in punctuation: \n",
    "        tweet = tweet.replace(p, \"\") # Removes punctiation\n",
    "    tweet = word_tokenize(tweet) # Creates a list of words\n",
    "    words = \"\"\n",
    "    for each_word in tweet:\n",
    "        if each_word not in terms_to_remove:\n",
    "            words = words + \" \" + each_word\n",
    "    # return [word for word in tweet if word not in terms_to_remove]\n",
    "    return words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session, load the test dataset into a Spark DataFrame and then adjust column names\n",
    "from pyspark.sql import SparkSession, functions\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Training Twitter Sentiment Analysis\").getOrCreate()\n",
    "test_data = spark.read.load(\n",
    "    \"tmp/testdata.manual.2009.06.14.csv\",\n",
    "    format=\"csv\")\n",
    "test_data = test_data.withColumnRenamed(\"_c0\", \"label\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"tweet_id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"query\") \\\n",
    "    .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "    .withColumnRenamed(\"_c5\", \"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the cleansing UDF for tweet column\n",
    "udf_cleansing = functions.udf(cleansing)\n",
    "test_data = test_data.withColumn(\"tweet_cleansed\", udf_cleansing(functions.col(\"tweet\")))\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "The following code snippet creates a list of every remaining word (after cleansing) that will be used to build the features for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|               words|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|[loooooooovvvvvve...|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|[reading, kindle2...|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|[ok, first, asses...|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|[youll, love, kin...|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|[fair, enough, ki...|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_cleansed\", outputCol=\"words\")\n",
    "test_data = tokenizer.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashingTF\n",
    "\n",
    "The following code snippet creates the features, which means a list of all terms available on all \"documents\" with a \"tag\" indicating its presence or not on the specific \"document\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|               words|            features|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|[loooooooovvvvvve...|(262144,[12524,83...|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|[reading, kindle2...|(262144,[53570,73...|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|[ok, first, asses...|(262144,[41748,12...|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|[youll, love, kin...|(262144,[1546,218...|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|[fair, enough, ki...|(262144,[32392,11...|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "test_data = hashingTF.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|               words|            features|labelIndex|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|[loooooooovvvvvve...|(262144,[12524,83...|       0.0|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|[reading, kindle2...|(262144,[53570,73...|       0.0|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|[ok, first, asses...|(262144,[41748,12...|       0.0|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|[youll, love, kin...|(262144,[1546,218...|       0.0|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|[fair, enough, ki...|(262144,[32392,11...|       0.0|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a numeric index for the labels\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "model = stringIndexer.fit(test_data)\n",
    "test_data = model.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved model\n",
    "\n",
    "Here we load the saved model that we have previously persisted on disk and use it to classify our test set (which is different from the one we used to evaluate when modeling). It is important that the DataFrame being used have the same columns (features and labelIndex) that we used when training it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment_spark\")\n",
    "if not os.path.exists(model_folder):\n",
    "    print(\"model does not exists\")\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayesModel\n",
    "loadModel = NaiveBayesModel.load(model_full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying using saved modelk\n",
    "predicted = loadModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+-------+\n",
      "|label|               tweet|labelIndex|NB_pred|\n",
      "+-----+--------------------+----------+-------+\n",
      "|    4|@stellargirl I lo...|       0.0|    1.0|\n",
      "|    4|Reading my kindle...|       0.0|    1.0|\n",
      "|    4|Ok, first assesme...|       0.0|    1.0|\n",
      "|    4|@kenburbary You'l...|       0.0|    0.0|\n",
      "|    4|@mikefish  Fair e...|       0.0|    1.0|\n",
      "+-----+--------------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.select(\"label\", \"tweet\", \"labelIndex\", \"NB_pred\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 498 \n",
      "Correct: 99 \n",
      "Accuracy: 0.19879518072289157\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the results with test dataset\n",
    "# It is important to note that our training set didn't have any Neutral (polarity = 2) single case\n",
    "total = predicted.count()\n",
    "correct = predicted.where(predicted['labelIndex'] == predicted['NB_pred']).count()\n",
    "accuracy = correct/total\n",
    "print(\n",
    "    \"\\nTotal:\", total, \n",
    "    \"\\nCorrect:\", correct, \n",
    "    \"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the initial label\n",
    "\n",
    "As we needed to create an index for our labels, we now need to \"translate\" back the predicted label index to our label. So, we firts create a \"domain\" table with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|label_id|label_predicted|\n",
      "+--------+---------------+\n",
      "|     0.0|              4|\n",
      "|     2.0|              2|\n",
      "|     1.0|              0|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = predicted.select(\"labelIndex\", \"label\").distinct() \\\n",
    "    .withColumnRenamed(\"label\", \"label_predicted\") \\\n",
    "    .withColumnRenamed(\"labelIndex\", \"label_id\")\n",
    "labels.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we join our \"domain\" table back to get the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predicted.join(labels, predicted[\"NB_pred\"] == labels[\"label_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+--------------+--------------------+---------------+--------------------+\n",
      "|label|tweet_id|                date|          user|               tweet|label_predicted|             NB_prob|\n",
      "+-----+--------+--------------------+--------------+--------------------+---------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|        tpryan|@stellargirl I lo...|              0|[0.04233691469917...|\n",
      "|    4|       4|Mon May 11 03:18:...|        vcu451|Reading my kindle...|              0|[0.06596172796309...|\n",
      "|    4|       5|Mon May 11 03:18:...|        chadfu|Ok, first assesme...|              0|[0.46934826833763...|\n",
      "|    4|       6|Mon May 11 03:19:...|         SIX15|@kenburbary You'l...|              4|[0.55688630507323...|\n",
      "|    4|       7|Mon May 11 03:21:...|      yamarama|@mikefish  Fair e...|              0|[0.34454700914178...|\n",
      "|    4|       8|Mon May 11 03:22:...|  GeorgeVHulme|@richardebaker no...|              0|[0.12654217460118...|\n",
      "|    0|       9|Mon May 11 03:22:...|       Seth937|Fuck this economy...|              4|[0.99221265924479...|\n",
      "|    4|      10|Mon May 11 03:26:...|     dcostalis|Jquery is my new ...|              0|[0.20341674363468...|\n",
      "|    4|      11|Mon May 11 03:27:...|       PJ_King|       Loves twitter|              0|[0.11544098611086...|\n",
      "|    4|      12|Mon May 11 03:29:...|   mandanicole|how can you not l...|              0|[0.05783076536473...|\n",
      "|    2|      13|Mon May 11 03:32:...|          jpeb|Check this video ...|              0|[0.11873667270502...|\n",
      "|    0|      14|Mon May 11 03:32:...|   kylesellers|@Karoli I firmly ...|              4|[0.63977831563340...|\n",
      "|    4|      15|Mon May 11 03:33:...|   theviewfans|House Corresponde...|              0|[0.44157297670716...|\n",
      "|    4|      16|Mon May 11 05:05:...|        MumsFP|Watchin Espn..Jus...|              4|[0.70595549431664...|\n",
      "|    0|      17|Mon May 11 05:06:...|   vincentx24x|dear nike, stop w...|              4|[0.91346360565752...|\n",
      "|    4|      18|Mon May 11 05:20:...|  cameronwylie|#lebron best athl...|              4|[0.62397869779476...|\n",
      "|    0|      19|Mon May 11 05:20:...|       luv8242|I was talking to ...|              4|[0.99518591573986...|\n",
      "|    4|      20|Mon May 11 05:21:...|    mtgillikin|i love lebron. ht...|              0|[0.45030765679766...|\n",
      "|    0|      21|Mon May 11 05:21:...|ursecretdezire|@ludajuice Lebron...|              4|[0.76563078577859...|\n",
      "|    4|      22|Mon May 11 05:21:...|     Native_01|@Pmillzz lebron I...|              4|[0.59377276571847...|\n",
      "+-----+--------+--------------------+--------------+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.select(\"label\", \"tweet_id\", \"date\", \"user\", \"tweet\", \"label_predicted\", \"NB_prob\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary folder\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

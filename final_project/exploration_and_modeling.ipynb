{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request \n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file on a temporary folder\n",
    "temporary_folder = os.path.join(os.getcwd(), 'tmp')\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)\n",
    "    \n",
    "if not os.path.exists(temporary_folder):\n",
    "    os.makedirs(temporary_folder)\n",
    "    \n",
    "local_file_name = local_file_name = os.path.join(base_folder, \"training_dataset\", \"trainingandtestdata.zip\")\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(local_file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temporary_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset\n",
    "\n",
    "The following function loads the training file and split it into training and test datasets\n",
    "\n",
    "It received the following args:\n",
    "\n",
    "* **sample_size**: the amount of rows from the file that we want to load. The whole file has 1.6MM of rows and it is unpractical to work with this amount on a local machine. For the final training with the whole dataset, a Hadoop cliuster are advised. If the arg is not informed, the function will return all the lines into two lists of dicts: one for training and another for testing\n",
    "* **test_size_frac**: the fraction of lines that will be reserved for testing the model\n",
    "\n",
    "**Note**: we are converting the Pandas DataFrame to a list of dict because nltk package does not work with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_dataset(sample_size = None, test_size_frac = 0.5):\n",
    "    training_dataset_path = os.path.join(\n",
    "        temporary_folder, \n",
    "        \"training.1600000.processed.noemoticon.csv\")\n",
    "\n",
    "    training_dataset = pd.read_csv(\n",
    "        training_dataset_path, \n",
    "        encoding=\"latin-1\", \n",
    "        warn_bad_lines=True,\n",
    "        error_bad_lines=False,\n",
    "        header=None, \n",
    "        names=[\"polarity\", \"tweet_id\", \"date\", \"query\", \"user\", \"tweet\"])\n",
    "    if sample_size != None:\n",
    "        training_dataset = training_dataset.sample(sample_size)\n",
    "\n",
    "    #training_dataset = training_dataset[[\"tweet_id\", \"polarity\", \"tweet\"]]\n",
    "    \n",
    "    testing_dataset = training_dataset.sample(frac = test_size_frac)\n",
    "\n",
    "    training_dataset = training_dataset.drop(testing_dataset.index)\n",
    " \n",
    "    return training_dataset.to_dict(\"records\"), testing_dataset.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and training dataset for exploration\n",
    "training_data, testing_data = load_training_dataset(sample_size = None, test_size_frac=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812025</td>\n",
       "      <td>Mon Apr 06 22:20:09 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mimismo</td>\n",
       "      <td>@twittera que me muera ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812784</td>\n",
       "      <td>Mon Apr 06 22:20:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bayofwolves</td>\n",
       "      <td>@smarrison i would've been the first, but i di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812964</td>\n",
       "      <td>Mon Apr 06 22:20:22 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lovesongwriter</td>\n",
       "      <td>Hollis' death scene will hurt me severely to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813137</td>\n",
       "      <td>Mon Apr 06 22:20:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>armotley</td>\n",
       "      <td>about to file taxes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467813579</td>\n",
       "      <td>Mon Apr 06 22:20:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>starkissed</td>\n",
       "      <td>@LettyA ahh ive always wanted to see rent  lov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                          date     query  \\\n",
       "0         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "2         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "4         0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "5         0  1467812025  Mon Apr 06 22:20:09 PDT 2009  NO_QUERY   \n",
       "6         0  1467812784  Mon Apr 06 22:20:20 PDT 2009  NO_QUERY   \n",
       "7         0  1467812964  Mon Apr 06 22:20:22 PDT 2009  NO_QUERY   \n",
       "8         0  1467813137  Mon Apr 06 22:20:25 PDT 2009  NO_QUERY   \n",
       "9         0  1467813579  Mon Apr 06 22:20:31 PDT 2009  NO_QUERY   \n",
       "\n",
       "             user                                              tweet  \n",
       "0   scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "1         ElleCTF    my whole body feels itchy and like its on fire   \n",
       "2          Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "3        joy_wolf                      @Kwesidei not the whole crew   \n",
       "4         mybirch                                        Need a hug   \n",
       "5         mimismo                          @twittera que me muera ?   \n",
       "6     bayofwolves  @smarrison i would've been the first, but i di...  \n",
       "7  lovesongwriter  Hollis' death scene will hurt me severely to w...  \n",
       "8        armotley                               about to file taxes   \n",
       "9      starkissed  @LettyA ahh ive always wanted to see rent  lov...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_data).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column definitions:\n",
    "\n",
    "0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "1 - the id of the tweet\n",
    "\n",
    "2 - the date of the tweet\n",
    "\n",
    "3 - the query. If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "5 - the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We noticed that there is not a single case of Neutral (2) polarity\n",
    "pd.DataFrame(training_data + testing_data).polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweets\n",
    "\n",
    "The following class prepares the dataset by:\n",
    "\n",
    "* Extracting the text from HTML (for the training dataset provided, we already have the text, but we want to avoid using any HTML tag for classification\n",
    "* Converting all words to lower case\n",
    "* Replacing any URL with \"URL\" constant (to enable the removal of them on a further step)\n",
    "* Replacing any tagging of users with \"USERTAGGING\" (to enable the removal of them in a further step)\n",
    "* Removing any \"#\" from hashtags\n",
    "* Removing punctuation (has little or no weight on classification as it can be used for both intentions)\n",
    "* Tokenizing (create a list of words)\n",
    "* And finally, removing words and punctuation that has little or no weight on classification (and can even create biases):\n",
    "    * Stop words: set of common words that are used doesn't matter the intenttion (things like it, that, a, the)\n",
    "    * Remove the two constants that we used to replace user tagging and URLs\n",
    "    \n",
    "**Note**: we are creating a class for this process because we want to \"pickle\" (serialize and save as a file) it for usage on the implementation of the streaming process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words(\"english\") + [\"USERTAGGING\",\"URL\"])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append(\n",
    "                (\n",
    "                    self.processTweet(tweet[\"tweet\"]),\n",
    "                    tweet[\"polarity\"]                    \n",
    "                )\n",
    "            )\n",
    "        return processedTweets\n",
    "    \n",
    "    def processTweet(self, tweet):\n",
    "        tweet = BeautifulSoup(tweet).get_text() # Extracts text from HTML (just in case!)\n",
    "        tweet = tweet.lower() # Converts text to lower-case\n",
    "        tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet) # Replces URLs by URL constan\n",
    "        tweet = re.sub(\"@[^\\s]+\", \"USERTAGGING\", tweet) # Replaces usernames by USERTAGGING constant \n",
    "        tweet = re.sub(r\"#([^\\s]+)\", r\"\\1\", tweet) # Removes the # in #hashtag\n",
    "        for p in punctuation: \n",
    "            tweet = tweet.replace(p, \"\") # Removes punctiation\n",
    "        tweet = word_tokenize(tweet) # Creates a list of words\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and training dataset for modeling\n",
    "training_data, testing_data = load_training_dataset(sample_size = 10000, test_size_frac=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Tweets\n",
    "tweet_processor = PreProcessTweets()\n",
    "pp_training_data = tweet_processor.processTweets(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['dont', 'like', 'thunder'], 0),\n",
       " (['killer', 'boobs', 'got', 'great', 'b', 'horror', 'flick'], 4),\n",
       " (['nice', 'day'], 4),\n",
       " (['half', 'hour', 'battleand', 'died', 'round', 'two', 'coming'], 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look on how some tweets look like after cleansing and tokenization\n",
    "pp_training_data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary\n",
    "\n",
    "The function below builds the vocabulary, it means the list of all words that we are going to use to train our model and later use to evaluate the tweet\n",
    "\n",
    "Some people argues that it is better to focus on the most used words (e.g. 2500 most used in our training dataset) and/or the words more present on documents (in our case tweets - like the words that are more present in more tweets)\n",
    "\n",
    "For the sake of this project, as it is not focused on the assertiveness of the model itself, but in the implementation of a pipeline using a model, we are going to use all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def build_vocabulary(preprocessed_training_dataset):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, polarity) in preprocessed_training_dataset:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    word_list = nltk.FreqDist(all_words)\n",
    "    word_features = word_list.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we build our vocabulary\n",
    "word_features = build_vocabulary(pp_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dont',\n",
       " 'like',\n",
       " 'thunder',\n",
       " 'killer',\n",
       " 'boobs',\n",
       " 'got',\n",
       " 'great',\n",
       " 'b',\n",
       " 'horror',\n",
       " 'flick']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's take a look on it:\n",
    "list(word_features)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Features\n",
    "The function below needs to be called for each one of the tweets and basically tags (with True) on a instance of the dictionary previously built which words in that instance of the dictionary that are used in that specific tweet. Thus, the majority of words will ba tagged as False and a small number of them (the ones contained in the tweet) as True \n",
    "\n",
    "**To-do**: this should also be encapsulated on a class in order to have it pickled. Or maybe encapsulate the whole code?!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word]=(word in tweet_words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the training features\n",
    "training_features = nltk.classify.apply_features(extract_features,pp_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'contains(dont)': True, 'contains(like)': True, 'contains(thunder)': True, 'contains(killer)': False, 'contains(boobs)': False, 'contains(got)': False, 'contains(great)': False, 'contains(b)': False, 'contains(horror)': False, 'contains(flick)': False, 'contains(nice)': False, 'contains(day)': False, 'contains(half)': False, 'contains(hour)': False, 'contains(battleand)': False, 'contains(died)': False, 'contains(round)': False, 'contains(two)': False, 'contains(coming)': False, 'contains(fell)': False, 'contains(asleep)': False, 'contains(kind)': False, 'contains(early)': False, 'contains(im)': False, 'contains(watching)': False, 'contains(shooter)': False, 'contains(drinkin)': False, 'contains(hot)': False, 'contains(choco)': False, 'contains(haha)': False, 'contains(stop)': False, 'contains(swear)': False, 'contains(lil)': False, 'contains(night)': False, 'contains(owl)': False, 'contains(get)': False, 'contains(much)': False, 'contains(done)': False, 'contains(use)': False, 'con ...\n"
     ]
    }
   ],
   "source": [
    "# And taking a look into it\n",
    "print(str(list(training_features)[:1])[0:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "And finally, we are going to train the model using Naive Bayes. We could have tried other classification algorithms but again, the main purpose of this project is the implementation of the pipeline, not the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "The following code uses the model trained to classify each one of the tweets of our testing dataset\n",
    "\n",
    "Note that before we do the classification, we need to apply the preprocess (cleansing and tokenizing) that we have built before and extract the features using our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for each_tweet in testing_data:\n",
    "    words = tweet_processor.processTweet(each_tweet[\"tweet\"])\n",
    "    row = {\n",
    "        \"polarity\": each_tweet[\"polarity\"],\n",
    "        \"tweet_id\": each_tweet[\"tweet_id\"],\n",
    "        \"date\": each_tweet[\"date\"],\n",
    "        \"query\": each_tweet[\"query\"],\n",
    "        \"user\": each_tweet[\"user\"],\n",
    "        \"tweet\": each_tweet[\"tweet\"],\n",
    "        \"predicted\": NBayesClassifier.classify(extract_features(words))\n",
    "    }\n",
    "\n",
    "    li.append(row)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet just creates a Pandas DataFrame with the results of our prediction along with some variables that we are going to use on our evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2058386299</td>\n",
       "      <td>Sat Jun 06 14:50:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>dragonzigg</td>\n",
       "      <td>Rabbit's eye infection has got worse. He's goi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2045222266</td>\n",
       "      <td>Fri Jun 05 10:24:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ChariseB</td>\n",
       "      <td>voiceover world, here I come!  Oh yeah, and le...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2002522484</td>\n",
       "      <td>Tue Jun 02 04:27:29 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>rebeccawatson</td>\n",
       "      <td>@iszi_lawrence Bike's trashed.  Hoping the guy...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2249194715</td>\n",
       "      <td>Fri Jun 19 21:53:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>youngcobris</td>\n",
       "      <td>DUDE WHAT'S THE POINT OF GOING DOWNTOWN WHEN Y...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1556723428</td>\n",
       "      <td>Sun Apr 19 00:02:39 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OhFlip_Eden</td>\n",
       "      <td>Listening to basshunter and looking at my pics...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>2286340958</td>\n",
       "      <td>Mon Jun 22 16:18:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Tzortze</td>\n",
       "      <td>RIP @cnnbrk : 4 dead in D.C Metro train colli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>2032450969</td>\n",
       "      <td>Thu Jun 04 11:16:31 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mrs_pacman</td>\n",
       "      <td>@puggylicious I really want to but my scanner ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4</td>\n",
       "      <td>1833800417</td>\n",
       "      <td>Mon May 18 00:30:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>IAmAliciaMarie</td>\n",
       "      <td>Yay I finished another book. I am on a roll</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>1982245543</td>\n",
       "      <td>Sun May 31 10:49:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>myria101</td>\n",
       "      <td>@Mkenn076 bc they're back together. I cried fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>2004545649</td>\n",
       "      <td>Tue Jun 02 08:22:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xjessukkax</td>\n",
       "      <td>going to cookville for my granny's for 2 days ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      polarity    tweet_id                          date     query  \\\n",
       "0            0  2058386299  Sat Jun 06 14:50:15 PDT 2009  NO_QUERY   \n",
       "1            4  2045222266  Fri Jun 05 10:24:53 PDT 2009  NO_QUERY   \n",
       "2            0  2002522484  Tue Jun 02 04:27:29 PDT 2009  NO_QUERY   \n",
       "3            0  2249194715  Fri Jun 19 21:53:20 PDT 2009  NO_QUERY   \n",
       "4            4  1556723428  Sun Apr 19 00:02:39 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "4995         0  2286340958  Mon Jun 22 16:18:51 PDT 2009  NO_QUERY   \n",
       "4996         0  2032450969  Thu Jun 04 11:16:31 PDT 2009  NO_QUERY   \n",
       "4997         4  1833800417  Mon May 18 00:30:20 PDT 2009  NO_QUERY   \n",
       "4998         0  1982245543  Sun May 31 10:49:23 PDT 2009  NO_QUERY   \n",
       "4999         0  2004545649  Tue Jun 02 08:22:05 PDT 2009  NO_QUERY   \n",
       "\n",
       "                user                                              tweet  \\\n",
       "0         dragonzigg  Rabbit's eye infection has got worse. He's goi...   \n",
       "1           ChariseB  voiceover world, here I come!  Oh yeah, and le...   \n",
       "2      rebeccawatson  @iszi_lawrence Bike's trashed.  Hoping the guy...   \n",
       "3        youngcobris  DUDE WHAT'S THE POINT OF GOING DOWNTOWN WHEN Y...   \n",
       "4        OhFlip_Eden  Listening to basshunter and looking at my pics...   \n",
       "...              ...                                                ...   \n",
       "4995         Tzortze   RIP @cnnbrk : 4 dead in D.C Metro train colli...   \n",
       "4996      mrs_pacman  @puggylicious I really want to but my scanner ...   \n",
       "4997  IAmAliciaMarie       Yay I finished another book. I am on a roll    \n",
       "4998        myria101  @Mkenn076 bc they're back together. I cried fo...   \n",
       "4999      xjessukkax  going to cookville for my granny's for 2 days ...   \n",
       "\n",
       "      predicted  \n",
       "0             0  \n",
       "1             4  \n",
       "2             4  \n",
       "3             4  \n",
       "4             0  \n",
       "...         ...  \n",
       "4995          0  \n",
       "4996          0  \n",
       "4997          4  \n",
       "4998          0  \n",
       "4999          0  \n",
       "\n",
       "[5000 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset = pd.DataFrame(li)\n",
    "Y_test = final_dataset[\"polarity\"]\n",
    "predicted = final_dataset[\"predicted\"]\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Confusion Matrix (just reminding that we did not use the whole training dataset, just a sample of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1706  734]\n",
      " [ 751 1809]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here our classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70      2440\n",
      "           4       0.71      0.71      0.71      2560\n",
      "\n",
      "    accuracy                           0.70      5000\n",
      "   macro avg       0.70      0.70      0.70      5000\n",
      "weighted avg       0.70      0.70      0.70      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just extracting the precision (with more precision....hahaha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      " 0.703\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\\n\", accuracy_score(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary folder\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

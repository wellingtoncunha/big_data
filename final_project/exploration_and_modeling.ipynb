{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib.request \n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file on a temporary folder\n",
    "temporary_folder = os.path.join(os.getcwd(), 'tmp')\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)\n",
    "    \n",
    "if not os.path.exists(temporary_folder):\n",
    "    os.makedirs(temporary_folder)\n",
    "    \n",
    "local_file_name = os.path.join(base_folder, \"training_dataset\", \"trainingandtestdata.zip\")\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(local_file_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(temporary_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Dataset\n",
    "\n",
    "The following function loads the training file and split it into training and test datasets\n",
    "\n",
    "It received the following args:\n",
    "\n",
    "* **sample_size**: the amount of rows from the file that we want to load. The whole file has 1.6MM of rows and it is unpractical to work with this amount on a local machine. For the final training with the whole dataset, a Hadoop cliuster are advised. If the arg is not informed, the function will return all the lines into two lists of dicts: one for training and another for testing\n",
    "* **test_size_frac**: the fraction of lines that will be reserved for testing the model\n",
    "\n",
    "**Note**: we are converting the Pandas DataFrame to a list of dict because nltk package does not work with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_dataset(sample_size = None, test_size_frac = 0.5):\n",
    "    training_dataset_path = os.path.join(\n",
    "        temporary_folder, \n",
    "        \"training.1600000.processed.noemoticon.csv\")\n",
    "\n",
    "    training_dataset = pd.read_csv(\n",
    "        training_dataset_path, \n",
    "        encoding=\"latin-1\", \n",
    "        warn_bad_lines=True,\n",
    "        error_bad_lines=False,\n",
    "        header=None, \n",
    "        names=[\"polarity\", \"tweet_id\", \"date\", \"query\", \"user\", \"tweet\"])\n",
    "    if sample_size != None:\n",
    "        training_dataset = training_dataset.sample(sample_size)\n",
    "\n",
    "    #training_dataset = training_dataset[[\"tweet_id\", \"polarity\", \"tweet\"]]\n",
    "    \n",
    "    testing_dataset = training_dataset.sample(frac = test_size_frac)\n",
    "\n",
    "    training_dataset = training_dataset.drop(testing_dataset.index)\n",
    " \n",
    "    return training_dataset.to_dict(\"records\"), testing_dataset.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and training dataset for exploration\n",
    "training_data, testing_data = load_training_dataset(sample_size = None, test_size_frac=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811592</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mybirch</td>\n",
       "      <td>Need a hug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811594</td>\n",
       "      <td>Mon Apr 06 22:20:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>coZZ</td>\n",
       "      <td>@LOLTrish hey  long time no see! Yes.. Rains a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811795</td>\n",
       "      <td>Mon Apr 06 22:20:05 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>2Hood4Hollywood</td>\n",
       "      <td>@Tatiana_K nope they didn't have it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1467812579</td>\n",
       "      <td>Mon Apr 06 22:20:17 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>pardonlauren</td>\n",
       "      <td>I just re-pierced my ears</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity    tweet_id                          date     query  \\\n",
       "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1         0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2         0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3         0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4         0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "5         0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "6         0  1467811592  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "7         0  1467811594  Mon Apr 06 22:20:03 PDT 2009  NO_QUERY   \n",
       "8         0  1467811795  Mon Apr 06 22:20:05 PDT 2009  NO_QUERY   \n",
       "9         0  1467812579  Mon Apr 06 22:20:17 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "5         joy_wolf                      @Kwesidei not the whole crew   \n",
       "6          mybirch                                        Need a hug   \n",
       "7             coZZ  @LOLTrish hey  long time no see! Yes.. Rains a...  \n",
       "8  2Hood4Hollywood               @Tatiana_K nope they didn't have it   \n",
       "9     pardonlauren                         I just re-pierced my ears   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(training_data).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column definitions:\n",
    "\n",
    "0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "1 - the id of the tweet\n",
    "\n",
    "2 - the date of the tweet\n",
    "\n",
    "3 - the query. If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "5 - the text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We noticed that there is not a single case of Neutral (2) polarity\n",
    "pd.DataFrame(training_data + testing_data).polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process Tweets\n",
    "\n",
    "The following class prepares the dataset by:\n",
    "\n",
    "* Extracting the text from HTML (for the training dataset provided, we already have the text, but we want to avoid using any HTML tag for classification\n",
    "* Converting all words to lower case\n",
    "* Replacing any URL with \"URL\" constant (to enable the removal of them on a further step)\n",
    "* Replacing any tagging of users with \"USERTAGGING\" (to enable the removal of them in a further step)\n",
    "* Removing any \"#\" from hashtags\n",
    "* Removing punctuation (has little or no weight on classification as it can be used for both intentions)\n",
    "* Tokenizing (create a list of words)\n",
    "* And finally, removing words and punctuation that has little or no weight on classification (and can even create biases):\n",
    "    * Stop words: set of common words that are used doesn't matter the intenttion (things like it, that, a, the)\n",
    "    * Remove the two constants that we used to replace user tagging and URLs\n",
    "    \n",
    "**Note**: we are creating a class for this process because we want to \"pickle\" (serialize and save as a file) it for usage on the implementation of the streaming process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class PreProcessTweets:\n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words(\"english\") + [\"USERTAGGING\",\"URL\"])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processedTweets.append(\n",
    "                (\n",
    "                    self.processTweet(tweet[\"tweet\"]),\n",
    "                    tweet[\"polarity\"]                    \n",
    "                )\n",
    "            )\n",
    "        return processedTweets\n",
    "    \n",
    "    def processTweet(self, tweet):\n",
    "        tweet = BeautifulSoup(tweet).get_text() # Extracts text from HTML (just in case!)\n",
    "        tweet = tweet.lower() # Converts text to lower-case\n",
    "        tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet) # Replces URLs by URL constan\n",
    "        tweet = re.sub(\"@[^\\s]+\", \"USERTAGGING\", tweet) # Replaces usernames by USERTAGGING constant \n",
    "        tweet = re.sub(r\"#([^\\s]+)\", r\"\\1\", tweet) # Removes the # in #hashtag\n",
    "        for p in punctuation: \n",
    "            tweet = tweet.replace(p, \"\") # Removes punctiation\n",
    "        tweet = word_tokenize(tweet) # Creates a list of words\n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test and training dataset for modeling\n",
    "training_data, testing_data = load_training_dataset(sample_size = 10000, test_size_frac=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Tweets\n",
    "tweet_processor = PreProcessTweets()\n",
    "pp_training_data = tweet_processor.processTweets(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['afternoon',\n",
       "   'took',\n",
       "   'boys',\n",
       "   'swimming',\n",
       "   'morning',\n",
       "   'lovely',\n",
       "   'pink',\n",
       "   'hair',\n",
       "   'gone',\n",
       "   'yellow'],\n",
       "  0),\n",
       " (['trying', 'get', 'swine', 'flu'], 4),\n",
       " (['lol'], 4),\n",
       " (['guys',\n",
       "   'always',\n",
       "   'hang',\n",
       "   'conserv',\n",
       "   'never',\n",
       "   'invite',\n",
       "   'hahaha',\n",
       "   'emo',\n",
       "   'shiz',\n",
       "   'hi',\n",
       "   'charles'],\n",
       "  0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look on how some tweets look like after cleansing and tokenization\n",
    "pp_training_data[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary\n",
    "\n",
    "The function below builds the vocabulary, it means the list of all words that we are going to use to train our model and later use to evaluate the tweet\n",
    "\n",
    "Some people argues that it is better to focus on the most used words (e.g. 2500 most used in our training dataset) and/or the words more present on documents (in our case tweets - like the words that are more present in more tweets)\n",
    "\n",
    "For the sake of this project, as it is not focused on the assertiveness of the model itself, but in the implementation of a pipeline using a model, we are going to use all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "def build_vocabulary(preprocessed_training_dataset):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, polarity) in preprocessed_training_dataset:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    word_list = nltk.FreqDist(all_words)\n",
    "    word_features = word_list.keys()\n",
    "    \n",
    "    return word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we build our vocabulary\n",
    "word_features = build_vocabulary(pp_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['afternoon',\n",
       " 'took',\n",
       " 'boys',\n",
       " 'swimming',\n",
       " 'morning',\n",
       " 'lovely',\n",
       " 'pink',\n",
       " 'hair',\n",
       " 'gone',\n",
       " 'yellow']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and let's take a look on it:\n",
    "list(word_features)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Features\n",
    "The function below needs to be called for each one of the tweets and basically tags (with True) on a instance of the dictionary previously built which words in that instance of the dictionary that are used in that specific tweet. Thus, the majority of words will ba tagged as False and a small number of them (the ones contained in the tweet) as True \n",
    "\n",
    "**To-do**: this should also be encapsulated on a class in order to have it pickled. Or maybe encapsulate the whole code?!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet):\n",
    "    tweet_words=set(tweet)\n",
    "    features={}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word]=(word in tweet_words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the training features\n",
    "training_features = nltk.classify.apply_features(extract_features,pp_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'contains(afternoon)': True, 'contains(took)': True, 'contains(boys)': True, 'contains(swimming)': True, 'contains(morning)': True, 'contains(lovely)': True, 'contains(pink)': True, 'contains(hair)': True, 'contains(gone)': True, 'contains(yellow)': True, 'contains(trying)': False, 'contains(get)': False, 'contains(swine)': False, 'contains(flu)': False, 'contains(lol)': False, 'contains(guys)': False, 'contains(always)': False, 'contains(hang)': False, 'contains(conserv)': False, 'contains(never)': False, 'contains(invite)': False, 'contains(hahaha)': False, 'contains(emo)': False, 'contains(shiz)': False, 'contains(hi)': False, 'contains(charles)': False, 'contains(believe)': False, 'contains(better)': False, 'contains(nick)': False, 'contains(matter)': False, 'contains(sounds)': False, 'contains(much)': False, 'contains(burntwell)': False, 'contains(would)': False, 'contains(wiser)': False, 'contains(put)': False, 'contains(salad)': False, 'contains(dressing)': False, 'contains(n ...\n"
     ]
    }
   ],
   "source": [
    "# And taking a look into it\n",
    "print(str(list(training_features)[:1])[0:1000], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "And finally, we are going to train the model using Naive Bayes. We could have tried other classification algorithms but again, the main purpose of this project is the implementation of the pipeline, not the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model\n",
    "The following code uses the model trained to classify each one of the tweets of our testing dataset\n",
    "\n",
    "Note that before we do the classification, we need to apply the preprocess (cleansing and tokenizing) that we have built before and extract the features using our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "threshold = 0  #.65\n",
    "for each_tweet in testing_data:\n",
    "    words = tweet_processor.processTweet(each_tweet[\"tweet\"])\n",
    "    features = extract_features(words)\n",
    "    predicted = NBayesClassifier.classify(features)\n",
    "    probability = NBayesClassifier.prob_classify(features).prob(predicted)\n",
    "    row = {\n",
    "        \"polarity\": each_tweet[\"polarity\"],\n",
    "        \"tweet_id\": each_tweet[\"tweet_id\"],\n",
    "        \"date\": each_tweet[\"date\"],\n",
    "        \"query\": each_tweet[\"query\"],\n",
    "        \"user\": each_tweet[\"user\"],\n",
    "        \"tweet\": each_tweet[\"tweet\"],\n",
    "        \"predicted\": predicted if probability > threshold else 2,\n",
    "        \"probability\": probability\n",
    "    }\n",
    "\n",
    "    li.append(row)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet just creates a Pandas DataFrame with the results of our prediction along with some variables that we are going to use on our evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>predicted</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1752963165</td>\n",
       "      <td>Sat May 09 23:36:37 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>iMartha182</td>\n",
       "      <td>well i guess it all depends,undergarments.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.531430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2262731169</td>\n",
       "      <td>Sat Jun 20 23:06:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hisydneyxo</td>\n",
       "      <td>cant sleep. blaring inseparable, just friends,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.977596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2048547569</td>\n",
       "      <td>Fri Jun 05 15:22:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>megan_spfan</td>\n",
       "      <td>@beckyscherger aww... i'm so sorry about your ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2175246659</td>\n",
       "      <td>Mon Jun 15 00:28:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>laura_dolcepics</td>\n",
       "      <td>@MsYuri I know what you mean about bad sleep s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.871236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2004969209</td>\n",
       "      <td>Tue Jun 02 09:01:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mishhhx3</td>\n",
       "      <td>Ok... I lied. Mini tacos for breakfast: not so...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.996309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4</td>\n",
       "      <td>1565430530</td>\n",
       "      <td>Mon Apr 20 06:44:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>wipeout_ut</td>\n",
       "      <td>@BearMate hard to be productive on Mondays.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.523514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>1694562733</td>\n",
       "      <td>Mon May 04 03:11:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lenje</td>\n",
       "      <td>@Silverlines Oh well, though I don't like meeb...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.968952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4</td>\n",
       "      <td>2063449641</td>\n",
       "      <td>Sun Jun 07 02:42:21 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>thinkadrian</td>\n",
       "      <td>Where can I see live #eu09 results on the inte...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.795563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4</td>\n",
       "      <td>1962308800</td>\n",
       "      <td>Fri May 29 10:50:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>niamhsmith</td>\n",
       "      <td>@mrelihan oh hey thanks for the recommendation...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.995011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4</td>\n",
       "      <td>1880184875</td>\n",
       "      <td>Fri May 22 00:22:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Misses_Gola</td>\n",
       "      <td>trotzdem guten morgen an alle  #followfriday</td>\n",
       "      <td>4</td>\n",
       "      <td>0.982202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      polarity    tweet_id                          date     query  \\\n",
       "0            4  1752963165  Sat May 09 23:36:37 PDT 2009  NO_QUERY   \n",
       "1            0  2262731169  Sat Jun 20 23:06:48 PDT 2009  NO_QUERY   \n",
       "2            0  2048547569  Fri Jun 05 15:22:44 PDT 2009  NO_QUERY   \n",
       "3            4  2175246659  Mon Jun 15 00:28:10 PDT 2009  NO_QUERY   \n",
       "4            4  2004969209  Tue Jun 02 09:01:06 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "4995         4  1565430530  Mon Apr 20 06:44:25 PDT 2009  NO_QUERY   \n",
       "4996         0  1694562733  Mon May 04 03:11:12 PDT 2009  NO_QUERY   \n",
       "4997         4  2063449641  Sun Jun 07 02:42:21 PDT 2009  NO_QUERY   \n",
       "4998         4  1962308800  Fri May 29 10:50:47 PDT 2009  NO_QUERY   \n",
       "4999         4  1880184875  Fri May 22 00:22:25 PDT 2009  NO_QUERY   \n",
       "\n",
       "                 user                                              tweet  \\\n",
       "0          iMartha182        well i guess it all depends,undergarments.    \n",
       "1          hisydneyxo  cant sleep. blaring inseparable, just friends,...   \n",
       "2         megan_spfan  @beckyscherger aww... i'm so sorry about your ...   \n",
       "3     laura_dolcepics  @MsYuri I know what you mean about bad sleep s...   \n",
       "4            mishhhx3  Ok... I lied. Mini tacos for breakfast: not so...   \n",
       "...               ...                                                ...   \n",
       "4995       wipeout_ut      @BearMate hard to be productive on Mondays.     \n",
       "4996            lenje  @Silverlines Oh well, though I don't like meeb...   \n",
       "4997      thinkadrian  Where can I see live #eu09 results on the inte...   \n",
       "4998       niamhsmith  @mrelihan oh hey thanks for the recommendation...   \n",
       "4999      Misses_Gola       trotzdem guten morgen an alle  #followfriday   \n",
       "\n",
       "      predicted  probability  \n",
       "0             0     0.531430  \n",
       "1             4     0.977596  \n",
       "2             0     0.800908  \n",
       "3             0     0.871236  \n",
       "4             4     0.996309  \n",
       "...         ...          ...  \n",
       "4995          0     0.523514  \n",
       "4996          0     0.968952  \n",
       "4997          4     0.795563  \n",
       "4998          4     0.995011  \n",
       "4999          4     0.982202  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset = pd.DataFrame(li)\n",
    "Y_test = final_dataset[\"polarity\"]\n",
    "predicted = final_dataset[\"predicted\"]\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the Confusion Matrix (just reminding that we did not use the whole training dataset, just a sample of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1760  744]\n",
      " [ 727 1769]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here our classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.70      0.71      2504\n",
      "           4       0.70      0.71      0.71      2496\n",
      "\n",
      "    accuracy                           0.71      5000\n",
      "   macro avg       0.71      0.71      0.71      5000\n",
      "weighted avg       0.71      0.71      0.71      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\\n\", classification_report(Y_test,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just extracting the precision (with more precision....hahaha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:\n",
      " 0.7058\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\\n\", accuracy_score(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "    \n",
    "if not os.path.exists(model_folder):\n",
    "    os.makedirs(model_folder)\n",
    "    \n",
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(NBayesClassifier, open(model_full_path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pickle = pickle.load(open(model_full_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trotzdem', 'guten', 'morgen', 'alle', 'followfriday'] 4 0.9822019301460024\n"
     ]
    }
   ],
   "source": [
    "features = extract_features(words)\n",
    "predicted = test_pickle.classify(features)\n",
    "probability = test_pickle.prob_classify(features).prob(predicted)\n",
    "\n",
    "print(words, predicted, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2</td>\n",
       "      <td>14072</td>\n",
       "      <td>Sun Jun 14 04:31:43 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>proggit</td>\n",
       "      <td>Ask Programming: LaTeX or InDesign?: submitted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0</td>\n",
       "      <td>14073</td>\n",
       "      <td>Sun Jun 14 04:32:17 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>sam33r</td>\n",
       "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>4</td>\n",
       "      <td>14074</td>\n",
       "      <td>Sun Jun 14 04:36:34 UTC 2009</td>\n",
       "      <td>latex</td>\n",
       "      <td>iamtheonlyjosie</td>\n",
       "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>14075</td>\n",
       "      <td>Sun Jun 14 21:36:07 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>plutopup7</td>\n",
       "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0</td>\n",
       "      <td>14076</td>\n",
       "      <td>Sun Jun 14 21:36:17 UTC 2009</td>\n",
       "      <td>iran</td>\n",
       "      <td>captain_pete</td>\n",
       "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>498 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     polarity  tweet_id                          date    query  \\\n",
       "0           4         3  Mon May 11 03:17:40 UTC 2009  kindle2   \n",
       "1           4         4  Mon May 11 03:18:03 UTC 2009  kindle2   \n",
       "2           4         5  Mon May 11 03:18:54 UTC 2009  kindle2   \n",
       "3           4         6  Mon May 11 03:19:04 UTC 2009  kindle2   \n",
       "4           4         7  Mon May 11 03:21:41 UTC 2009  kindle2   \n",
       "..        ...       ...                           ...      ...   \n",
       "493         2     14072  Sun Jun 14 04:31:43 UTC 2009    latex   \n",
       "494         0     14073  Sun Jun 14 04:32:17 UTC 2009    latex   \n",
       "495         4     14074  Sun Jun 14 04:36:34 UTC 2009    latex   \n",
       "496         0     14075  Sun Jun 14 21:36:07 UTC 2009     iran   \n",
       "497         0     14076  Sun Jun 14 21:36:17 UTC 2009     iran   \n",
       "\n",
       "                user                                              tweet  \n",
       "0             tpryan  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1             vcu451  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2             chadfu  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3              SIX15  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4           yamarama  @mikefish  Fair enough. But i have the Kindle2...  \n",
       "..               ...                                                ...  \n",
       "493          proggit  Ask Programming: LaTeX or InDesign?: submitted...  \n",
       "494           sam33r  On that note, I hate Word. I hate Pages. I hat...  \n",
       "495  iamtheonlyjosie  Ahhh... back in a *real* text editing environm...  \n",
       "496        plutopup7  Trouble in Iran, I see. Hmm. Iran. Iran so far...  \n",
       "497     captain_pete  Reading the tweets coming out of Iran... The w...  \n",
       "\n",
       "[498 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset_path = os.path.join(\n",
    "    temporary_folder, \n",
    "    \"testdata.manual.2009.06.14.csv\")\n",
    "\n",
    "test_dataset = pd.read_csv(\n",
    "    test_dataset_path, \n",
    "    encoding=\"latin-1\", \n",
    "    warn_bad_lines=True,\n",
    "    error_bad_lines=False,\n",
    "    header=None, \n",
    "    names=[\"polarity\", \"tweet_id\", \"date\", \"query\", \"user\", \"tweet\"])\n",
    "\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary folder\n",
    "if os.path.exists(temporary_folder):\n",
    "    shutil.rmtree(temporary_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

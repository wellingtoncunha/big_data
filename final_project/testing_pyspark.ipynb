{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.getcwd()\n",
    "temporary_folder = os.path.join(os.getcwd(), \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files():\n",
    "# Unzip file on a temporary folder\n",
    "    if os.path.exists(temporary_folder):\n",
    "        shutil.rmtree(temporary_folder)\n",
    "        \n",
    "    if not os.path.exists(temporary_folder):\n",
    "        os.makedirs(temporary_folder)\n",
    "        \n",
    "    local_file_name = os.path.join(base_folder, \"training_dataset\", \"trainingandtestdata.zip\")\n",
    "    with zipfile.ZipFile(local_file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temporary_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing_and_tokenizing(tweet):\n",
    "# Cleansing and tokenizing tweet\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.corpus import stopwords \n",
    "    from string import punctuation \n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    terms_to_remove = set(stopwords.words(\"english\") + [\"USERTAGGING\",\"URL\"])\n",
    "    tweet = BeautifulSoup(tweet, 'html.parser').get_text() # Extracts text from HTML (just in case!)\n",
    "    tweet = tweet.lower() # Converts text to lower-case\n",
    "    tweet = re.sub(\"((www\\.[^\\s]+)|(https?://[^\\s]+))\", \"URL\", tweet) # Replces URLs by URL constan\n",
    "    tweet = re.sub(\"@[^\\s]+\", \"USERTAGGING\", tweet) # Replaces usernames by USERTAGGING constant \n",
    "    tweet = re.sub(r\"#([^\\s]+)\", r\"\\1\", tweet) # Removes the # in #hashtag\n",
    "    for p in punctuation: \n",
    "        tweet = tweet.replace(p, \"\") # Removes punctiation\n",
    "    tweet = word_tokenize(tweet) # Creates a list of words\n",
    "    words = \"\"\n",
    "    for each_word in tweet:\n",
    "        if each_word not in terms_to_remove:\n",
    "            words = words + \" \" + each_word\n",
    "    # return [word for word in tweet if word not in terms_to_remove]\n",
    "    return words[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Training Twitter Sentiment Analysis\").getOrCreate()\n",
    "test_data = spark.read.load(\n",
    "    \"tmp/testdata.manual.2009.06.14.csv\",\n",
    "    format=\"csv\")\n",
    "test_data = test_data.withColumnRenamed(\"_c0\", \"label\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"tweet_id\") \\\n",
    "    .withColumnRenamed(\"_c2\", \"date\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"query\") \\\n",
    "    .withColumnRenamed(\"_c4\", \"user\") \\\n",
    "    .withColumnRenamed(\"_c5\", \"tweet\")\n",
    "test_data = test_data.withColumn(\"label\", functions.col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_cleansing_and_tokenizing = functions.udf(cleansing_and_tokenizing)\n",
    "test_data = test_data.withColumn(\"tweet_cleansed\", udf_cleansing_and_tokenizing(functions.col(\"tweet\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_cleansed\", outputCol=\"words\")\n",
    "test_data = tokenizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|               words|           term_freq|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|[loooooooovvvvvve...|(262144,[12524,83...|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|[reading, kindle2...|(262144,[53570,73...|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|[ok, first, asses...|(262144,[41748,12...|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|[youll, love, kin...|(262144,[1546,218...|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|[fair, enough, ki...|(262144,[32392,11...|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"term_freq\")\n",
    "test_data = hashingTF.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|               words|           term_freq|               tfidf|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|[loooooooovvvvvve...|(262144,[12524,83...|(262144,[12524,83...|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|[reading, kindle2...|(262144,[53570,73...|(262144,[53570,73...|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|[ok, first, asses...|(262144,[41748,12...|(262144,[41748,12...|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|[youll, love, kin...|(262144,[1546,218...|(262144,[1546,218...|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|[fair, enough, ki...|(262144,[32392,11...|(262144,[32392,11...|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF \n",
    "idf = IDF(inputCol=\"term_freq\", outputCol=\"tfidf\")\n",
    "idfModel = idf.fit(test_data)\n",
    "test_data = idfModel.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|tweet_id|                date|  query|    user|               tweet|      tweet_cleansed|               words|           term_freq|               tfidf|labelIndex|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    4|       3|Mon May 11 03:17:...|kindle2|  tpryan|@stellargirl I lo...|loooooooovvvvvvee...|[loooooooovvvvvve...|(262144,[12524,83...|(262144,[12524,83...|       0.0|\n",
      "|    4|       4|Mon May 11 03:18:...|kindle2|  vcu451|Reading my kindle...|reading kindle2 l...|[reading, kindle2...|(262144,[53570,73...|(262144,[53570,73...|       0.0|\n",
      "|    4|       5|Mon May 11 03:18:...|kindle2|  chadfu|Ok, first assesme...|ok first assesmen...|[ok, first, asses...|(262144,[41748,12...|(262144,[41748,12...|       0.0|\n",
      "|    4|       6|Mon May 11 03:19:...|kindle2|   SIX15|@kenburbary You'l...|youll love kindle...|[youll, love, kin...|(262144,[1546,218...|(262144,[1546,218...|       0.0|\n",
      "|    4|       7|Mon May 11 03:21:...|kindle2|yamarama|@mikefish  Fair e...|fair enough kindl...|[fair, enough, ki...|(262144,[32392,11...|(262144,[32392,11...|       0.0|\n",
      "+-----+--------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"labelIndex\")\n",
    "model = stringIndexer.fit(test_data)\n",
    "test_data = model.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               tfidf|label|\n",
      "+--------------------+-----+\n",
      "|(262144,[12524,83...|    4|\n",
      "|(262144,[53570,73...|    4|\n",
      "|(262144,[41748,12...|    4|\n",
      "|(262144,[1546,218...|    4|\n",
      "|(262144,[32392,11...|    4|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = test_data.select(\"tfidf\", \"label\")\n",
    "predicted.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o401.load.\n: java.lang.NoSuchMethodException: org.apache.spark.ml.classification.NaiveBayesModel.<init>(java.lang.String)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-c505904625fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloadModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_full_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o401.load.\n: java.lang.NoSuchMethodException: org.apache.spark.ml.classification.NaiveBayesModel.<init>(java.lang.String)\n\tat java.base/java.lang.Class.getConstructor0(Class.java:3349)\n\tat java.base/java.lang.Class.getConstructor(Class.java:2151)\n\tat org.apache.spark.ml.util.DefaultParamsReader.load(ReadWrite.scala:468)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment_spark\")\n",
    "if not os.path.exists(model_folder):\n",
    "    print(\"model does not exists\")\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "loadModel = NaiveBayes.load(model_full_path)\n",
    "predicted = loadModel.transform(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-922dc0cb4a4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model does not exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpersistedModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_full_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;34m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultParamsReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'language'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paramMap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paramMap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mJavaMLReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipelineSharedReadWrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/ml/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path should be a string, got type %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mjava_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_from_java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             raise NotImplementedError(\"This Java ML type cannot be loaded into Python currently: %r\"\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: requirement failed: Error loading metadata: Expected class name org.apache.spark.ml.PipelineModel but found class name org.apache.spark.ml.classification.NaiveBayes"
     ]
    }
   ],
   "source": [
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment_spark\")\n",
    "if not os.path.exists(model_folder):\n",
    "    print(\"model does not exists\")\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "persistedModel = PipelineModel.load(model_full_path)\n",
    "predicted = persistedModel.transform(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "#Naive bayes\n",
    "nb = NaiveBayes(featuresCol=\"tfidf\", labelCol=\"labelIndex\", predictionCol=\"NB_pred\",\n",
    "                probabilityCol=\"NB_prob\", rawPredictionCol=\"NB_rawPred\")\n",
    "nbModel = nb.fit(training)\n",
    "cv = nbModel.transform(test)\n",
    "total = cv.count()\n",
    "correct = cv.where(test['labelIndex'] == cv['NB_pred']).count()\n",
    "accuracy = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 10105 \n",
      "Correct: 6875 \n",
      "Accuracy: 0.6803562592775854\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"\\nTotal:\", total, \n",
    "    \"\\nCorrect:\", correct, \n",
    "    \"\\nAccuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "\n",
    "    os.makedirs(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full_path = os.path.join(model_folder, \"twitter_sentiment_spark.pk\")\n",
    "import shutil\n",
    "shutil.rmtree(model_full_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.save(model_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------+\n",
      "|               tfidf|labelIndex|          NB_rawPred|             NB_prob|NB_pred|\n",
      "+--------------------+----------+--------------------+--------------------+-------+\n",
      "|(262144,[18,42059...|       1.0|[-252.72281533249...|[0.13013379281339...|    1.0|\n",
      "|(262144,[42,3386,...|       1.0|[-375.86466509495...|[0.03119412210768...|    1.0|\n",
      "|(262144,[81,39216...|       1.0|[-440.84634663110...|[0.99999942585461...|    0.0|\n",
      "|(262144,[101,3437...|       1.0|[-332.86250322387...|[0.59229784306963...|    0.0|\n",
      "|(262144,[104,1758...|       1.0|[-591.18590544612...|[0.00701729009458...|    1.0|\n",
      "|(262144,[104,2436...|       1.0|[-453.58083478952...|[5.44060865770748...|    1.0|\n",
      "|(262144,[119,8804...|       1.0|[-552.86652919912...|[0.28571853232588...|    1.0|\n",
      "|(262144,[145,8929...|       1.0|[-658.59185497054...|[0.99999999999900...|    0.0|\n",
      "|(262144,[150,1002...|       1.0|[-529.54002759273...|[0.00407730535488...|    1.0|\n",
      "|(262144,[161,822,...|       1.0|[-920.12864989089...|[0.88373359638080...|    0.0|\n",
      "|(262144,[161,6533...|       0.0|[-704.16623324043...|[0.99999999999977...|    0.0|\n",
      "|(262144,[161,1072...|       1.0|[-1023.8715182929...|[0.99999999998934...|    0.0|\n",
      "|(262144,[161,2430...|       1.0|[-390.09025866324...|[9.1991957238845E...|    1.0|\n",
      "|(262144,[161,3101...|       1.0|[-440.76122763732...|[0.01938374680267...|    1.0|\n",
      "|(262144,[161,3434...|       1.0|[-175.68974042406...|[7.13932089032028...|    1.0|\n",
      "|(262144,[161,9265...|       1.0|[-262.73689644292...|[4.40980276048358...|    1.0|\n",
      "|(262144,[220,1512...|       1.0|[-441.76132301752...|[1.80526138934621...|    1.0|\n",
      "|(262144,[281,2032...|       1.0|[-553.44223572360...|[0.99999988846432...|    0.0|\n",
      "|(262144,[285,7096...|       1.0|[-514.65442301149...|[0.99999999999692...|    0.0|\n",
      "|(262144,[298,5619...|       1.0|[-920.44765757447...|[7.28535568286287...|    1.0|\n",
      "+--------------------+----------+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               tfidf|labelIndex|\n",
      "+--------------------+----------+\n",
      "|(262144,[18,42059...|       1.0|\n",
      "|(262144,[42,3386,...|       1.0|\n",
      "|(262144,[81,39216...|       1.0|\n",
      "|(262144,[101,3437...|       1.0|\n",
      "|(262144,[104,1758...|       1.0|\n",
      "|(262144,[104,2436...|       1.0|\n",
      "|(262144,[119,8804...|       1.0|\n",
      "|(262144,[145,8929...|       1.0|\n",
      "|(262144,[150,1002...|       1.0|\n",
      "|(262144,[161,822,...|       1.0|\n",
      "|(262144,[161,6533...|       0.0|\n",
      "|(262144,[161,1072...|       1.0|\n",
      "|(262144,[161,2430...|       1.0|\n",
      "|(262144,[161,3101...|       1.0|\n",
      "|(262144,[161,3434...|       1.0|\n",
      "|(262144,[161,9265...|       1.0|\n",
      "|(262144,[220,1512...|       1.0|\n",
      "|(262144,[281,2032...|       1.0|\n",
      "|(262144,[285,7096...|       1.0|\n",
      "|(262144,[298,5619...|       1.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+-------+----+\n",
      "|               tfidf|labelIndex|          NB_rawPred|             NB_prob|NB_pred|id_b|\n",
      "+--------------------+----------+--------------------+--------------------+-------+----+\n",
      "|(262144,[18,42059...|       1.0|[-252.72281533249...|[0.13013379281339...|    1.0|   0|\n",
      "|(262144,[42,3386,...|       1.0|[-375.86466509495...|[0.03119412210768...|    1.0|   1|\n",
      "|(262144,[81,39216...|       1.0|[-440.84634663110...|[0.99999942585461...|    0.0|   2|\n",
      "|(262144,[101,3437...|       1.0|[-332.86250322387...|[0.59229784306963...|    0.0|   3|\n",
      "|(262144,[104,1758...|       1.0|[-591.18590544612...|[0.00701729009458...|    1.0|   4|\n",
      "|(262144,[104,2436...|       1.0|[-453.58083478952...|[5.44060865770748...|    1.0|   5|\n",
      "|(262144,[119,8804...|       1.0|[-552.86652919912...|[0.28571853232588...|    1.0|   6|\n",
      "|(262144,[145,8929...|       1.0|[-658.59185497054...|[0.99999999999900...|    0.0|   7|\n",
      "|(262144,[150,1002...|       1.0|[-529.54002759273...|[0.00407730535488...|    1.0|   8|\n",
      "|(262144,[161,822,...|       1.0|[-920.12864989089...|[0.88373359638080...|    0.0|   9|\n",
      "|(262144,[161,6533...|       0.0|[-704.16623324043...|[0.99999999999977...|    0.0|  10|\n",
      "|(262144,[161,1072...|       1.0|[-1023.8715182929...|[0.99999999998934...|    0.0|  11|\n",
      "|(262144,[161,2430...|       1.0|[-390.09025866324...|[9.1991957238845E...|    1.0|  12|\n",
      "|(262144,[161,3101...|       1.0|[-440.76122763732...|[0.01938374680267...|    1.0|  13|\n",
      "|(262144,[161,3434...|       1.0|[-175.68974042406...|[7.13932089032028...|    1.0|  14|\n",
      "|(262144,[161,9265...|       1.0|[-262.73689644292...|[4.40980276048358...|    1.0|  15|\n",
      "|(262144,[220,1512...|       1.0|[-441.76132301752...|[1.80526138934621...|    1.0|  16|\n",
      "|(262144,[281,2032...|       1.0|[-553.44223572360...|[0.99999988846432...|    0.0|  17|\n",
      "|(262144,[285,7096...|       1.0|[-514.65442301149...|[0.99999999999692...|    0.0|  18|\n",
      "|(262144,[298,5619...|       1.0|[-920.44765757447...|[7.28535568286287...|    1.0|  19|\n",
      "+--------------------+----------+--------------------+--------------------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "cv_indexed = cv.select(\"*\").withColumn(\"id_b\", monotonically_increasing_id())\n",
    "\n",
    "cv_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----+\n",
      "|               tfidf|labelIndex|id_b|\n",
      "+--------------------+----------+----+\n",
      "|(262144,[18,42059...|       1.0|   0|\n",
      "|(262144,[42,3386,...|       1.0|   1|\n",
      "|(262144,[81,39216...|       1.0|   2|\n",
      "|(262144,[101,3437...|       1.0|   3|\n",
      "|(262144,[104,1758...|       1.0|   4|\n",
      "|(262144,[104,2436...|       1.0|   5|\n",
      "|(262144,[119,8804...|       1.0|   6|\n",
      "|(262144,[145,8929...|       1.0|   7|\n",
      "|(262144,[150,1002...|       1.0|   8|\n",
      "|(262144,[161,822,...|       1.0|   9|\n",
      "|(262144,[161,6533...|       0.0|  10|\n",
      "|(262144,[161,1072...|       1.0|  11|\n",
      "|(262144,[161,2430...|       1.0|  12|\n",
      "|(262144,[161,3101...|       1.0|  13|\n",
      "|(262144,[161,3434...|       1.0|  14|\n",
      "|(262144,[161,9265...|       1.0|  15|\n",
      "|(262144,[220,1512...|       1.0|  16|\n",
      "|(262144,[281,2032...|       1.0|  17|\n",
      "|(262144,[285,7096...|       1.0|  18|\n",
      "|(262144,[298,5619...|       1.0|  19|\n",
      "+--------------------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id \n",
    "test_indexed = test.select(\"*\").withColumn(\"id_b\", monotonically_increasing_id())\n",
    "\n",
    "test_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----+--------------------+----------+--------------------+--------------------+-------+----+\n",
      "|               tfidf|labelIndex|id_b|               tfidf|labelIndex|          NB_rawPred|             NB_prob|NB_pred|id_b|\n",
      "+--------------------+----------+----+--------------------+----------+--------------------+--------------------+-------+----+\n",
      "|(262144,[18,42059...|       1.0|   0|(262144,[18,42059...|       1.0|[-252.72281533249...|[0.13013379281339...|    1.0|   0|\n",
      "|(262144,[42,3386,...|       1.0|   1|(262144,[42,3386,...|       1.0|[-375.86466509495...|[0.03119412210768...|    1.0|   1|\n",
      "|(262144,[81,39216...|       1.0|   2|(262144,[81,39216...|       1.0|[-440.84634663110...|[0.99999942585461...|    0.0|   2|\n",
      "|(262144,[101,3437...|       1.0|   3|(262144,[101,3437...|       1.0|[-332.86250322387...|[0.59229784306963...|    0.0|   3|\n",
      "|(262144,[104,1758...|       1.0|   4|(262144,[104,1758...|       1.0|[-591.18590544612...|[0.00701729009458...|    1.0|   4|\n",
      "|(262144,[104,2436...|       1.0|   5|(262144,[104,2436...|       1.0|[-453.58083478952...|[5.44060865770748...|    1.0|   5|\n",
      "|(262144,[119,8804...|       1.0|   6|(262144,[119,8804...|       1.0|[-552.86652919912...|[0.28571853232588...|    1.0|   6|\n",
      "|(262144,[145,8929...|       1.0|   7|(262144,[145,8929...|       1.0|[-658.59185497054...|[0.99999999999900...|    0.0|   7|\n",
      "|(262144,[150,1002...|       1.0|   8|(262144,[150,1002...|       1.0|[-529.54002759273...|[0.00407730535488...|    1.0|   8|\n",
      "|(262144,[161,822,...|       1.0|   9|(262144,[161,822,...|       1.0|[-920.12864989089...|[0.88373359638080...|    0.0|   9|\n",
      "|(262144,[161,6533...|       0.0|  10|(262144,[161,6533...|       0.0|[-704.16623324043...|[0.99999999999977...|    0.0|  10|\n",
      "|(262144,[161,1072...|       1.0|  11|(262144,[161,1072...|       1.0|[-1023.8715182929...|[0.99999999998934...|    0.0|  11|\n",
      "|(262144,[161,2430...|       1.0|  12|(262144,[161,2430...|       1.0|[-390.09025866324...|[9.1991957238845E...|    1.0|  12|\n",
      "|(262144,[161,3101...|       1.0|  13|(262144,[161,3101...|       1.0|[-440.76122763732...|[0.01938374680267...|    1.0|  13|\n",
      "|(262144,[161,3434...|       1.0|  14|(262144,[161,3434...|       1.0|[-175.68974042406...|[7.13932089032028...|    1.0|  14|\n",
      "|(262144,[161,9265...|       1.0|  15|(262144,[161,9265...|       1.0|[-262.73689644292...|[4.40980276048358...|    1.0|  15|\n",
      "|(262144,[220,1512...|       1.0|  16|(262144,[220,1512...|       1.0|[-441.76132301752...|[1.80526138934621...|    1.0|  16|\n",
      "|(262144,[281,2032...|       1.0|  17|(262144,[281,2032...|       1.0|[-553.44223572360...|[0.99999988846432...|    0.0|  17|\n",
      "|(262144,[285,7096...|       1.0|  18|(262144,[285,7096...|       1.0|[-514.65442301149...|[0.99999999999692...|    0.0|  18|\n",
      "|(262144,[298,5619...|       1.0|  19|(262144,[298,5619...|       1.0|[-920.44765757447...|[7.28535568286287...|    1.0|  19|\n",
      "+--------------------+----------+----+--------------------+----------+--------------------+--------------------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# merged = test_indexed.join(cv_indexed, on=col(\"columnindex\"), how='inner') #.drop(\"id\") \n",
    "merged = test_indexed.join(cv_indexed, test_indexed.id_b == cv_indexed.id_b)\n",
    "merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
